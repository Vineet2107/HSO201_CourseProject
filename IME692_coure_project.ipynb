{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Vineet2107/HSO201_CourseProject/blob/main/IME692_coure_project.ipynb",
      "authorship_tag": "ABX9TyPBdu2iZrjsqFVbSLod50GB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vineet2107/HSO201_CourseProject/blob/main/IME692_coure_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "T8Mvkxosy3Db"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Course Project IME692/voweltrain_ .csv')\n",
        "df1=pd.read_csv('/content/drive/MyDrive/Course Project IME692/voweltest_.csv')"
      ],
      "metadata": {
        "id": "QY2_0C9Y0VpE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "xgBsfnZM03e9",
        "outputId": "6027aea8-76b0-4638-cf04-99b7c1d53111"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    row.names   y    x.1    x.2    x.3    x.4    x.5    x.6    x.7    x.8  \\\n",
              "0           1   1 -3.639  0.418 -0.670  1.779 -0.168  1.627 -0.388  0.529   \n",
              "1           2   2 -3.327  0.496 -0.694  1.365 -0.265  1.933 -0.363  0.510   \n",
              "2           3   3 -2.120  0.894 -1.576  0.147 -0.707  1.559 -0.579  0.676   \n",
              "3           4   4 -2.287  1.809 -1.498  1.012 -1.053  1.060 -0.567  0.235   \n",
              "4           5   5 -2.598  1.938 -0.846  1.062 -1.633  0.764  0.394 -0.150   \n",
              "5           6   6 -2.852  1.914 -0.755  0.825 -1.588  0.855  0.217 -0.246   \n",
              "6           7   7 -3.482  2.524 -0.433  1.048 -1.995  0.902  0.322  0.450   \n",
              "7           8   8 -3.941  2.305  0.124  1.771 -1.815  0.593 -0.435  0.992   \n",
              "8           9   9 -3.860  2.116 -0.939  0.688 -0.675  1.679 -0.512  0.928   \n",
              "9          10  10 -3.648  1.812 -1.378  1.578  0.065  1.577 -0.466  0.702   \n",
              "10         11  11 -3.032  1.739 -1.141  0.737 -0.834  1.386 -0.575  0.679   \n",
              "11         12   1 -3.653  0.373 -0.600  1.705 -0.222  1.765 -0.353  0.537   \n",
              "12         13   2 -3.237  0.436 -0.860  1.363 -0.251  1.915 -0.395  0.751   \n",
              "13         14   3 -2.135  0.954 -1.632  0.121 -0.704  1.600 -0.628  0.713   \n",
              "14         15   4 -2.304  1.784 -1.506  0.981 -0.961  0.806 -0.294 -0.002   \n",
              "\n",
              "      x.9   x.10  \n",
              "0  -0.874 -0.814  \n",
              "1  -0.621 -0.488  \n",
              "2  -0.809 -0.049  \n",
              "3  -0.091 -0.795  \n",
              "4   0.277 -0.396  \n",
              "5   0.238 -0.365  \n",
              "6   0.377 -0.366  \n",
              "7   0.575 -0.301  \n",
              "8  -0.167 -0.434  \n",
              "9   0.060 -0.836  \n",
              "10 -0.018 -0.823  \n",
              "11 -0.797 -0.813  \n",
              "12 -0.774 -0.327  \n",
              "13 -0.903 -0.027  \n",
              "14  0.119 -0.760  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1cbd2867-72c6-4057-ac74-a5ee57494c24\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row.names</th>\n",
              "      <th>y</th>\n",
              "      <th>x.1</th>\n",
              "      <th>x.2</th>\n",
              "      <th>x.3</th>\n",
              "      <th>x.4</th>\n",
              "      <th>x.5</th>\n",
              "      <th>x.6</th>\n",
              "      <th>x.7</th>\n",
              "      <th>x.8</th>\n",
              "      <th>x.9</th>\n",
              "      <th>x.10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.639</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-0.670</td>\n",
              "      <td>1.779</td>\n",
              "      <td>-0.168</td>\n",
              "      <td>1.627</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>0.529</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>-3.327</td>\n",
              "      <td>0.496</td>\n",
              "      <td>-0.694</td>\n",
              "      <td>1.365</td>\n",
              "      <td>-0.265</td>\n",
              "      <td>1.933</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.510</td>\n",
              "      <td>-0.621</td>\n",
              "      <td>-0.488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-2.120</td>\n",
              "      <td>0.894</td>\n",
              "      <td>-1.576</td>\n",
              "      <td>0.147</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>1.559</td>\n",
              "      <td>-0.579</td>\n",
              "      <td>0.676</td>\n",
              "      <td>-0.809</td>\n",
              "      <td>-0.049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-2.287</td>\n",
              "      <td>1.809</td>\n",
              "      <td>-1.498</td>\n",
              "      <td>1.012</td>\n",
              "      <td>-1.053</td>\n",
              "      <td>1.060</td>\n",
              "      <td>-0.567</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>-0.795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-2.598</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>1.062</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-0.396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-2.852</td>\n",
              "      <td>1.914</td>\n",
              "      <td>-0.755</td>\n",
              "      <td>0.825</td>\n",
              "      <td>-1.588</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.217</td>\n",
              "      <td>-0.246</td>\n",
              "      <td>0.238</td>\n",
              "      <td>-0.365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>-3.482</td>\n",
              "      <td>2.524</td>\n",
              "      <td>-0.433</td>\n",
              "      <td>1.048</td>\n",
              "      <td>-1.995</td>\n",
              "      <td>0.902</td>\n",
              "      <td>0.322</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.377</td>\n",
              "      <td>-0.366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>-3.941</td>\n",
              "      <td>2.305</td>\n",
              "      <td>0.124</td>\n",
              "      <td>1.771</td>\n",
              "      <td>-1.815</td>\n",
              "      <td>0.593</td>\n",
              "      <td>-0.435</td>\n",
              "      <td>0.992</td>\n",
              "      <td>0.575</td>\n",
              "      <td>-0.301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>-3.860</td>\n",
              "      <td>2.116</td>\n",
              "      <td>-0.939</td>\n",
              "      <td>0.688</td>\n",
              "      <td>-0.675</td>\n",
              "      <td>1.679</td>\n",
              "      <td>-0.512</td>\n",
              "      <td>0.928</td>\n",
              "      <td>-0.167</td>\n",
              "      <td>-0.434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>-3.648</td>\n",
              "      <td>1.812</td>\n",
              "      <td>-1.378</td>\n",
              "      <td>1.578</td>\n",
              "      <td>0.065</td>\n",
              "      <td>1.577</td>\n",
              "      <td>-0.466</td>\n",
              "      <td>0.702</td>\n",
              "      <td>0.060</td>\n",
              "      <td>-0.836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>-3.032</td>\n",
              "      <td>1.739</td>\n",
              "      <td>-1.141</td>\n",
              "      <td>0.737</td>\n",
              "      <td>-0.834</td>\n",
              "      <td>1.386</td>\n",
              "      <td>-0.575</td>\n",
              "      <td>0.679</td>\n",
              "      <td>-0.018</td>\n",
              "      <td>-0.823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.653</td>\n",
              "      <td>0.373</td>\n",
              "      <td>-0.600</td>\n",
              "      <td>1.705</td>\n",
              "      <td>-0.222</td>\n",
              "      <td>1.765</td>\n",
              "      <td>-0.353</td>\n",
              "      <td>0.537</td>\n",
              "      <td>-0.797</td>\n",
              "      <td>-0.813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>-3.237</td>\n",
              "      <td>0.436</td>\n",
              "      <td>-0.860</td>\n",
              "      <td>1.363</td>\n",
              "      <td>-0.251</td>\n",
              "      <td>1.915</td>\n",
              "      <td>-0.395</td>\n",
              "      <td>0.751</td>\n",
              "      <td>-0.774</td>\n",
              "      <td>-0.327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>-2.135</td>\n",
              "      <td>0.954</td>\n",
              "      <td>-1.632</td>\n",
              "      <td>0.121</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>1.600</td>\n",
              "      <td>-0.628</td>\n",
              "      <td>0.713</td>\n",
              "      <td>-0.903</td>\n",
              "      <td>-0.027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>4</td>\n",
              "      <td>-2.304</td>\n",
              "      <td>1.784</td>\n",
              "      <td>-1.506</td>\n",
              "      <td>0.981</td>\n",
              "      <td>-0.961</td>\n",
              "      <td>0.806</td>\n",
              "      <td>-0.294</td>\n",
              "      <td>-0.002</td>\n",
              "      <td>0.119</td>\n",
              "      <td>-0.760</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1cbd2867-72c6-4057-ac74-a5ee57494c24')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1cbd2867-72c6-4057-ac74-a5ee57494c24 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1cbd2867-72c6-4057-ac74-a5ee57494c24');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.y=df.y-1\n",
        "df1.y=df1.y-1"
      ],
      "metadata": {
        "id": "12_wI1Ob6a4_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AGCegBJY6fDi",
        "outputId": "3882a8e5-c052-4534-e42b-2e5d943f82a6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     row.names   y    x.1    x.2    x.3    x.4    x.5    x.6    x.7    x.8  \\\n",
              "0            1   0 -3.639  0.418 -0.670  1.779 -0.168  1.627 -0.388  0.529   \n",
              "1            2   1 -3.327  0.496 -0.694  1.365 -0.265  1.933 -0.363  0.510   \n",
              "2            3   2 -2.120  0.894 -1.576  0.147 -0.707  1.559 -0.579  0.676   \n",
              "3            4   3 -2.287  1.809 -1.498  1.012 -1.053  1.060 -0.567  0.235   \n",
              "4            5   4 -2.598  1.938 -0.846  1.062 -1.633  0.764  0.394 -0.150   \n",
              "..         ...  ..    ...    ...    ...    ...    ...    ...    ...    ...   \n",
              "523        524   6 -4.065  2.876 -0.856 -0.221 -0.533  0.232  0.855  0.633   \n",
              "524        525   7 -4.513  4.265 -1.477 -1.090  0.215  0.829  0.342  0.693   \n",
              "525        526   8 -4.651  4.246 -0.823 -0.831  0.666  0.546 -0.300  0.094   \n",
              "526        527   9 -5.034  4.993 -1.633 -0.285  0.398  0.181 -0.211 -0.508   \n",
              "527        528  10 -4.261  1.827 -0.482 -0.194  0.731  0.354 -0.478  0.050   \n",
              "\n",
              "       x.9   x.10  \n",
              "0   -0.874 -0.814  \n",
              "1   -0.621 -0.488  \n",
              "2   -0.809 -0.049  \n",
              "3   -0.091 -0.795  \n",
              "4    0.277 -0.396  \n",
              "..     ...    ...  \n",
              "523 -1.452  0.272  \n",
              "524 -0.601 -0.056  \n",
              "525 -1.343  0.185  \n",
              "526 -0.283  0.304  \n",
              "527 -0.112  0.321  \n",
              "\n",
              "[528 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69120365-0530-4dac-b00a-80d671ffcbc3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row.names</th>\n",
              "      <th>y</th>\n",
              "      <th>x.1</th>\n",
              "      <th>x.2</th>\n",
              "      <th>x.3</th>\n",
              "      <th>x.4</th>\n",
              "      <th>x.5</th>\n",
              "      <th>x.6</th>\n",
              "      <th>x.7</th>\n",
              "      <th>x.8</th>\n",
              "      <th>x.9</th>\n",
              "      <th>x.10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.639</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-0.670</td>\n",
              "      <td>1.779</td>\n",
              "      <td>-0.168</td>\n",
              "      <td>1.627</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>0.529</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.327</td>\n",
              "      <td>0.496</td>\n",
              "      <td>-0.694</td>\n",
              "      <td>1.365</td>\n",
              "      <td>-0.265</td>\n",
              "      <td>1.933</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.510</td>\n",
              "      <td>-0.621</td>\n",
              "      <td>-0.488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>-2.120</td>\n",
              "      <td>0.894</td>\n",
              "      <td>-1.576</td>\n",
              "      <td>0.147</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>1.559</td>\n",
              "      <td>-0.579</td>\n",
              "      <td>0.676</td>\n",
              "      <td>-0.809</td>\n",
              "      <td>-0.049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>-2.287</td>\n",
              "      <td>1.809</td>\n",
              "      <td>-1.498</td>\n",
              "      <td>1.012</td>\n",
              "      <td>-1.053</td>\n",
              "      <td>1.060</td>\n",
              "      <td>-0.567</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>-0.795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>-2.598</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>1.062</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-0.396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>524</td>\n",
              "      <td>6</td>\n",
              "      <td>-4.065</td>\n",
              "      <td>2.876</td>\n",
              "      <td>-0.856</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.633</td>\n",
              "      <td>-1.452</td>\n",
              "      <td>0.272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>525</td>\n",
              "      <td>7</td>\n",
              "      <td>-4.513</td>\n",
              "      <td>4.265</td>\n",
              "      <td>-1.477</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.829</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.693</td>\n",
              "      <td>-0.601</td>\n",
              "      <td>-0.056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>526</td>\n",
              "      <td>8</td>\n",
              "      <td>-4.651</td>\n",
              "      <td>4.246</td>\n",
              "      <td>-0.823</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>0.666</td>\n",
              "      <td>0.546</td>\n",
              "      <td>-0.300</td>\n",
              "      <td>0.094</td>\n",
              "      <td>-1.343</td>\n",
              "      <td>0.185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>527</td>\n",
              "      <td>9</td>\n",
              "      <td>-5.034</td>\n",
              "      <td>4.993</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.181</td>\n",
              "      <td>-0.211</td>\n",
              "      <td>-0.508</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>0.304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>528</td>\n",
              "      <td>10</td>\n",
              "      <td>-4.261</td>\n",
              "      <td>1.827</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.354</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>0.050</td>\n",
              "      <td>-0.112</td>\n",
              "      <td>0.321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>528 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69120365-0530-4dac-b00a-80d671ffcbc3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69120365-0530-4dac-b00a-80d671ffcbc3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69120365-0530-4dac-b00a-80d671ffcbc3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.multiclass import type_of_target\n",
        "y_logi=df.y\n",
        "type_of_target(y_logi)"
      ],
      "metadata": {
        "id": "ycecv13ekPjo",
        "outputId": "033f4ba7-5c40-42e1-f647-001b50270216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'multiclass'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = tf.keras.utils.to_categorical(df.y)\n",
        "y_test = tf.keras.utils.to_categorical(df1.y)"
      ],
      "metadata": {
        "id": "Y1i__nz33FKe"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7BWXJ8s3Gac",
        "outputId": "a52a41b8-d49b-4a3e-d859-25d24377ce5a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(528, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY9XgR7bE9DC",
        "outputId": "00d8aa7d-f908-4671-957e-6e64cfb5e4d2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(462, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I67TcHNf6NmG",
        "outputId": "ea7544aa-4cf1-41e7-81e5-3210639fa215"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=df[df.columns[2:12]]"
      ],
      "metadata": {
        "id": "s4qIf0ak3Nbz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1=df1[df1.columns[2:12]]"
      ],
      "metadata": {
        "id": "0evXTiPxFGAX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "uERc-Rav3s_u",
        "outputId": "c41ec55f-3110-465a-dd4b-14129a06a744"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       x.1    x.2    x.3    x.4    x.5    x.6    x.7    x.8    x.9   x.10\n",
              "0   -3.639  0.418 -0.670  1.779 -0.168  1.627 -0.388  0.529 -0.874 -0.814\n",
              "1   -3.327  0.496 -0.694  1.365 -0.265  1.933 -0.363  0.510 -0.621 -0.488\n",
              "2   -2.120  0.894 -1.576  0.147 -0.707  1.559 -0.579  0.676 -0.809 -0.049\n",
              "3   -2.287  1.809 -1.498  1.012 -1.053  1.060 -0.567  0.235 -0.091 -0.795\n",
              "4   -2.598  1.938 -0.846  1.062 -1.633  0.764  0.394 -0.150  0.277 -0.396\n",
              "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...\n",
              "523 -4.065  2.876 -0.856 -0.221 -0.533  0.232  0.855  0.633 -1.452  0.272\n",
              "524 -4.513  4.265 -1.477 -1.090  0.215  0.829  0.342  0.693 -0.601 -0.056\n",
              "525 -4.651  4.246 -0.823 -0.831  0.666  0.546 -0.300  0.094 -1.343  0.185\n",
              "526 -5.034  4.993 -1.633 -0.285  0.398  0.181 -0.211 -0.508 -0.283  0.304\n",
              "527 -4.261  1.827 -0.482 -0.194  0.731  0.354 -0.478  0.050 -0.112  0.321\n",
              "\n",
              "[528 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-466a618c-b845-4b95-822e-1b1739dd8b66\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x.1</th>\n",
              "      <th>x.2</th>\n",
              "      <th>x.3</th>\n",
              "      <th>x.4</th>\n",
              "      <th>x.5</th>\n",
              "      <th>x.6</th>\n",
              "      <th>x.7</th>\n",
              "      <th>x.8</th>\n",
              "      <th>x.9</th>\n",
              "      <th>x.10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-3.639</td>\n",
              "      <td>0.418</td>\n",
              "      <td>-0.670</td>\n",
              "      <td>1.779</td>\n",
              "      <td>-0.168</td>\n",
              "      <td>1.627</td>\n",
              "      <td>-0.388</td>\n",
              "      <td>0.529</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-3.327</td>\n",
              "      <td>0.496</td>\n",
              "      <td>-0.694</td>\n",
              "      <td>1.365</td>\n",
              "      <td>-0.265</td>\n",
              "      <td>1.933</td>\n",
              "      <td>-0.363</td>\n",
              "      <td>0.510</td>\n",
              "      <td>-0.621</td>\n",
              "      <td>-0.488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.120</td>\n",
              "      <td>0.894</td>\n",
              "      <td>-1.576</td>\n",
              "      <td>0.147</td>\n",
              "      <td>-0.707</td>\n",
              "      <td>1.559</td>\n",
              "      <td>-0.579</td>\n",
              "      <td>0.676</td>\n",
              "      <td>-0.809</td>\n",
              "      <td>-0.049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-2.287</td>\n",
              "      <td>1.809</td>\n",
              "      <td>-1.498</td>\n",
              "      <td>1.012</td>\n",
              "      <td>-1.053</td>\n",
              "      <td>1.060</td>\n",
              "      <td>-0.567</td>\n",
              "      <td>0.235</td>\n",
              "      <td>-0.091</td>\n",
              "      <td>-0.795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.598</td>\n",
              "      <td>1.938</td>\n",
              "      <td>-0.846</td>\n",
              "      <td>1.062</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-0.150</td>\n",
              "      <td>0.277</td>\n",
              "      <td>-0.396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>-4.065</td>\n",
              "      <td>2.876</td>\n",
              "      <td>-0.856</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.633</td>\n",
              "      <td>-1.452</td>\n",
              "      <td>0.272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>-4.513</td>\n",
              "      <td>4.265</td>\n",
              "      <td>-1.477</td>\n",
              "      <td>-1.090</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.829</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.693</td>\n",
              "      <td>-0.601</td>\n",
              "      <td>-0.056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>-4.651</td>\n",
              "      <td>4.246</td>\n",
              "      <td>-0.823</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>0.666</td>\n",
              "      <td>0.546</td>\n",
              "      <td>-0.300</td>\n",
              "      <td>0.094</td>\n",
              "      <td>-1.343</td>\n",
              "      <td>0.185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>-5.034</td>\n",
              "      <td>4.993</td>\n",
              "      <td>-1.633</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.181</td>\n",
              "      <td>-0.211</td>\n",
              "      <td>-0.508</td>\n",
              "      <td>-0.283</td>\n",
              "      <td>0.304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>-4.261</td>\n",
              "      <td>1.827</td>\n",
              "      <td>-0.482</td>\n",
              "      <td>-0.194</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.354</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>0.050</td>\n",
              "      <td>-0.112</td>\n",
              "      <td>0.321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>528 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-466a618c-b845-4b95-822e-1b1739dd8b66')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-466a618c-b845-4b95-822e-1b1739dd8b66 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-466a618c-b845-4b95-822e-1b1739dd8b66');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=np.array(x)\n",
        "x_test=np.array(x1)"
      ],
      "metadata": {
        "id": "fspdZpD03t91"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3OOsMSCGGdA",
        "outputId": "f877cec7-eab3-42c0-99db-8f261e39cfef"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(528, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdSJSuYLGKh1",
        "outputId": "7d69d5ff-da08-4e2d-8544-7c0116dbde91"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(462, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perms = np.random.permutation(528)"
      ],
      "metadata": {
        "id": "O5XfvtV8HndO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=x_train[perms]\n",
        "y_train=y_train[perms]\n",
        "y_logi=y_logi[perms]"
      ],
      "metadata": {
        "id": "2zmlEZvFH_8v"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qyEzVb2QH_5B"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LvvLwAi30XW",
        "outputId": "c3d3e967-d319-445c-ddae-28982719f92d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.693,  3.067, -0.273, ...,  1.347,  0.346,  0.36 ],\n",
              "       [-3.293,  0.93 ,  0.522, ..., -0.094, -0.132,  0.198],\n",
              "       [-4.158, -0.342,  0.9  , ..., -0.328, -1.52 , -0.498],\n",
              "       ...,\n",
              "       [-2.952,  0.777, -0.74 , ..., -0.082, -1.456,  0.616],\n",
              "       [-3.986,  2.325,  0.102, ...,  1.003,  0.566, -0.245],\n",
              "       [-2.53 ,  1.492, -0.936, ...,  1.149,  0.226, -0.77 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=x_train\n",
        "X_test=x_test"
      ],
      "metadata": {
        "id": "GB69PgSKGEQO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imported necessary packages for model\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Activation\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ATcrErwW31it"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = [x_train.shape[1]]"
      ],
      "metadata": {
        "id": "Gw_170YV5FKV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sequential MLP model \n",
        "model = tf.keras.Sequential([\n",
        "    layers.BatchNormalization(input_shape=input_shape),\n",
        "    layers.Dense(20, activation='relu'),\n",
        "    layers.Dropout(0.05),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(10, activation='relu'),\n",
        "    layers.Dropout(0.05),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(11, activation = 'sigmoid'),\n",
        "])\n"
      ],
      "metadata": {
        "id": "BT_TRWIs6uLr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compiling model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4nLaTOs76z1L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training model\n",
        "epochs = 500\n",
        "history = model.fit(x_train, y_train, validation_split=0.1, batch_size=16,\n",
        "                    epochs=epochs, verbose=1,\n",
        "                    #validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
        "print((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ATSu7pnl7JWi",
        "outputId": "499df71c-7928-4e34-c1ad-144f5741e8fd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "30/30 [==============================] - 2s 12ms/step - loss: 2.6894 - accuracy: 0.0947 - val_loss: 2.7976 - val_accuracy: 0.1132\n",
            "Epoch 2/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 2.4834 - accuracy: 0.1368 - val_loss: 2.4779 - val_accuracy: 0.0943\n",
            "Epoch 3/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 2.3217 - accuracy: 0.1768 - val_loss: 2.3154 - val_accuracy: 0.2075\n",
            "Epoch 4/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 2.2067 - accuracy: 0.2611 - val_loss: 2.1799 - val_accuracy: 0.2830\n",
            "Epoch 5/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 2.1115 - accuracy: 0.2695 - val_loss: 2.0695 - val_accuracy: 0.2830\n",
            "Epoch 6/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 2.0085 - accuracy: 0.2968 - val_loss: 1.9570 - val_accuracy: 0.3208\n",
            "Epoch 7/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.9619 - accuracy: 0.3242 - val_loss: 1.8547 - val_accuracy: 0.4340\n",
            "Epoch 8/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.8778 - accuracy: 0.3368 - val_loss: 1.7628 - val_accuracy: 0.5094\n",
            "Epoch 9/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.7928 - accuracy: 0.3832 - val_loss: 1.6850 - val_accuracy: 0.5094\n",
            "Epoch 10/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.7576 - accuracy: 0.3832 - val_loss: 1.6022 - val_accuracy: 0.5283\n",
            "Epoch 11/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.6705 - accuracy: 0.4611 - val_loss: 1.5346 - val_accuracy: 0.5660\n",
            "Epoch 12/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.6472 - accuracy: 0.4568 - val_loss: 1.4724 - val_accuracy: 0.5660\n",
            "Epoch 13/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.6014 - accuracy: 0.4442 - val_loss: 1.4317 - val_accuracy: 0.5849\n",
            "Epoch 14/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.5620 - accuracy: 0.4863 - val_loss: 1.3864 - val_accuracy: 0.5849\n",
            "Epoch 15/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.5570 - accuracy: 0.4842 - val_loss: 1.3551 - val_accuracy: 0.5660\n",
            "Epoch 16/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.4930 - accuracy: 0.4989 - val_loss: 1.3146 - val_accuracy: 0.6415\n",
            "Epoch 17/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.4831 - accuracy: 0.4758 - val_loss: 1.2770 - val_accuracy: 0.6604\n",
            "Epoch 18/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.4271 - accuracy: 0.5032 - val_loss: 1.2493 - val_accuracy: 0.6604\n",
            "Epoch 19/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.4183 - accuracy: 0.4863 - val_loss: 1.2210 - val_accuracy: 0.6415\n",
            "Epoch 20/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.4041 - accuracy: 0.5221 - val_loss: 1.1875 - val_accuracy: 0.6415\n",
            "Epoch 21/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.3376 - accuracy: 0.5347 - val_loss: 1.1655 - val_accuracy: 0.6604\n",
            "Epoch 22/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.3888 - accuracy: 0.5032 - val_loss: 1.1487 - val_accuracy: 0.6981\n",
            "Epoch 23/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.3206 - accuracy: 0.5305 - val_loss: 1.1161 - val_accuracy: 0.6981\n",
            "Epoch 24/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.2823 - accuracy: 0.5495 - val_loss: 1.0939 - val_accuracy: 0.6981\n",
            "Epoch 25/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.2829 - accuracy: 0.5453 - val_loss: 1.0740 - val_accuracy: 0.7358\n",
            "Epoch 26/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.2697 - accuracy: 0.5600 - val_loss: 1.0464 - val_accuracy: 0.7736\n",
            "Epoch 27/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.2883 - accuracy: 0.5453 - val_loss: 1.0186 - val_accuracy: 0.7547\n",
            "Epoch 28/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.2587 - accuracy: 0.5411 - val_loss: 0.9906 - val_accuracy: 0.7358\n",
            "Epoch 29/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.2182 - accuracy: 0.5747 - val_loss: 0.9881 - val_accuracy: 0.7736\n",
            "Epoch 30/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.1844 - accuracy: 0.5979 - val_loss: 0.9717 - val_accuracy: 0.7736\n",
            "Epoch 31/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.2078 - accuracy: 0.5811 - val_loss: 0.9432 - val_accuracy: 0.8113\n",
            "Epoch 32/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.2086 - accuracy: 0.5768 - val_loss: 0.9242 - val_accuracy: 0.7547\n",
            "Epoch 33/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.2535 - accuracy: 0.5305 - val_loss: 0.9044 - val_accuracy: 0.7547\n",
            "Epoch 34/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.2108 - accuracy: 0.5642 - val_loss: 0.8735 - val_accuracy: 0.7547\n",
            "Epoch 35/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.1620 - accuracy: 0.5768 - val_loss: 0.8636 - val_accuracy: 0.7547\n",
            "Epoch 36/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.1229 - accuracy: 0.6126 - val_loss: 0.8579 - val_accuracy: 0.7547\n",
            "Epoch 37/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.1282 - accuracy: 0.5958 - val_loss: 0.8468 - val_accuracy: 0.7547\n",
            "Epoch 38/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.1715 - accuracy: 0.5811 - val_loss: 0.8241 - val_accuracy: 0.8113\n",
            "Epoch 39/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.0440 - accuracy: 0.6400 - val_loss: 0.8110 - val_accuracy: 0.8113\n",
            "Epoch 40/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.1408 - accuracy: 0.5768 - val_loss: 0.7991 - val_accuracy: 0.8113\n",
            "Epoch 41/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.1069 - accuracy: 0.5958 - val_loss: 0.7861 - val_accuracy: 0.8302\n",
            "Epoch 42/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0449 - accuracy: 0.6295 - val_loss: 0.7894 - val_accuracy: 0.8302\n",
            "Epoch 43/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0620 - accuracy: 0.6274 - val_loss: 0.7472 - val_accuracy: 0.8302\n",
            "Epoch 44/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.1198 - accuracy: 0.5663 - val_loss: 0.7294 - val_accuracy: 0.8302\n",
            "Epoch 45/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.0776 - accuracy: 0.5937 - val_loss: 0.7324 - val_accuracy: 0.8302\n",
            "Epoch 46/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0535 - accuracy: 0.5916 - val_loss: 0.7149 - val_accuracy: 0.8491\n",
            "Epoch 47/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0688 - accuracy: 0.6021 - val_loss: 0.7084 - val_accuracy: 0.8302\n",
            "Epoch 48/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.0643 - accuracy: 0.6042 - val_loss: 0.7161 - val_accuracy: 0.8491\n",
            "Epoch 49/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9767 - accuracy: 0.6463 - val_loss: 0.7055 - val_accuracy: 0.8113\n",
            "Epoch 50/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0109 - accuracy: 0.6211 - val_loss: 0.6868 - val_accuracy: 0.8113\n",
            "Epoch 51/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0131 - accuracy: 0.6232 - val_loss: 0.6800 - val_accuracy: 0.8302\n",
            "Epoch 52/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0443 - accuracy: 0.6274 - val_loss: 0.6910 - val_accuracy: 0.8302\n",
            "Epoch 53/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0432 - accuracy: 0.6063 - val_loss: 0.6857 - val_accuracy: 0.8302\n",
            "Epoch 54/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0122 - accuracy: 0.6084 - val_loss: 0.6857 - val_accuracy: 0.8491\n",
            "Epoch 55/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.0000 - accuracy: 0.6337 - val_loss: 0.6708 - val_accuracy: 0.8679\n",
            "Epoch 56/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9734 - accuracy: 0.6000 - val_loss: 0.6712 - val_accuracy: 0.8491\n",
            "Epoch 57/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9555 - accuracy: 0.6442 - val_loss: 0.6719 - val_accuracy: 0.8113\n",
            "Epoch 58/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9867 - accuracy: 0.6189 - val_loss: 0.6450 - val_accuracy: 0.8491\n",
            "Epoch 59/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9071 - accuracy: 0.6589 - val_loss: 0.6413 - val_accuracy: 0.8679\n",
            "Epoch 60/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9177 - accuracy: 0.6611 - val_loss: 0.6164 - val_accuracy: 0.8679\n",
            "Epoch 61/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9191 - accuracy: 0.6779 - val_loss: 0.6091 - val_accuracy: 0.8679\n",
            "Epoch 62/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9909 - accuracy: 0.6232 - val_loss: 0.6189 - val_accuracy: 0.8302\n",
            "Epoch 63/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.0060 - accuracy: 0.6379 - val_loss: 0.6175 - val_accuracy: 0.8302\n",
            "Epoch 64/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8992 - accuracy: 0.6695 - val_loss: 0.6049 - val_accuracy: 0.8679\n",
            "Epoch 65/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9474 - accuracy: 0.6379 - val_loss: 0.5944 - val_accuracy: 0.8679\n",
            "Epoch 66/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8756 - accuracy: 0.6589 - val_loss: 0.5944 - val_accuracy: 0.8868\n",
            "Epoch 67/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8820 - accuracy: 0.6800 - val_loss: 0.5729 - val_accuracy: 0.8679\n",
            "Epoch 68/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8600 - accuracy: 0.6758 - val_loss: 0.5706 - val_accuracy: 0.8868\n",
            "Epoch 69/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8924 - accuracy: 0.6337 - val_loss: 0.5656 - val_accuracy: 0.8868\n",
            "Epoch 70/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9595 - accuracy: 0.6274 - val_loss: 0.5731 - val_accuracy: 0.8113\n",
            "Epoch 71/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9101 - accuracy: 0.6568 - val_loss: 0.5582 - val_accuracy: 0.8491\n",
            "Epoch 72/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9082 - accuracy: 0.6611 - val_loss: 0.5581 - val_accuracy: 0.8679\n",
            "Epoch 73/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8992 - accuracy: 0.6674 - val_loss: 0.5445 - val_accuracy: 0.8868\n",
            "Epoch 74/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0459 - accuracy: 0.6063 - val_loss: 0.5575 - val_accuracy: 0.8868\n",
            "Epoch 75/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8869 - accuracy: 0.6737 - val_loss: 0.5572 - val_accuracy: 0.8679\n",
            "Epoch 76/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9824 - accuracy: 0.6295 - val_loss: 0.5639 - val_accuracy: 0.8679\n",
            "Epoch 77/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 1.0275 - accuracy: 0.6295 - val_loss: 0.5530 - val_accuracy: 0.8679\n",
            "Epoch 78/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8862 - accuracy: 0.6716 - val_loss: 0.5456 - val_accuracy: 0.8679\n",
            "Epoch 79/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9416 - accuracy: 0.6337 - val_loss: 0.5327 - val_accuracy: 0.8868\n",
            "Epoch 80/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8420 - accuracy: 0.6863 - val_loss: 0.5203 - val_accuracy: 0.8679\n",
            "Epoch 81/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9239 - accuracy: 0.6526 - val_loss: 0.5281 - val_accuracy: 0.8679\n",
            "Epoch 82/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9207 - accuracy: 0.6884 - val_loss: 0.5219 - val_accuracy: 0.8679\n",
            "Epoch 83/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8451 - accuracy: 0.6926 - val_loss: 0.5125 - val_accuracy: 0.8868\n",
            "Epoch 84/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8864 - accuracy: 0.6653 - val_loss: 0.5102 - val_accuracy: 0.9057\n",
            "Epoch 85/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8417 - accuracy: 0.6905 - val_loss: 0.5080 - val_accuracy: 0.8868\n",
            "Epoch 86/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8713 - accuracy: 0.6611 - val_loss: 0.4949 - val_accuracy: 0.9057\n",
            "Epoch 87/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8562 - accuracy: 0.6674 - val_loss: 0.4978 - val_accuracy: 0.9057\n",
            "Epoch 88/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8400 - accuracy: 0.6926 - val_loss: 0.5137 - val_accuracy: 0.8679\n",
            "Epoch 89/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 1.0037 - accuracy: 0.6337 - val_loss: 0.5084 - val_accuracy: 0.8679\n",
            "Epoch 90/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8282 - accuracy: 0.6905 - val_loss: 0.5042 - val_accuracy: 0.8868\n",
            "Epoch 91/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8655 - accuracy: 0.6589 - val_loss: 0.5067 - val_accuracy: 0.8679\n",
            "Epoch 92/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9226 - accuracy: 0.6274 - val_loss: 0.5107 - val_accuracy: 0.8491\n",
            "Epoch 93/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9023 - accuracy: 0.6526 - val_loss: 0.5000 - val_accuracy: 0.8868\n",
            "Epoch 94/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8539 - accuracy: 0.6905 - val_loss: 0.5019 - val_accuracy: 0.8679\n",
            "Epoch 95/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8614 - accuracy: 0.6800 - val_loss: 0.4983 - val_accuracy: 0.8679\n",
            "Epoch 96/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8597 - accuracy: 0.6716 - val_loss: 0.4897 - val_accuracy: 0.8868\n",
            "Epoch 97/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9010 - accuracy: 0.6442 - val_loss: 0.4921 - val_accuracy: 0.8679\n",
            "Epoch 98/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8952 - accuracy: 0.6716 - val_loss: 0.4886 - val_accuracy: 0.8491\n",
            "Epoch 99/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8735 - accuracy: 0.6863 - val_loss: 0.4970 - val_accuracy: 0.8491\n",
            "Epoch 100/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8123 - accuracy: 0.6884 - val_loss: 0.4862 - val_accuracy: 0.8491\n",
            "Epoch 101/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8764 - accuracy: 0.6463 - val_loss: 0.4872 - val_accuracy: 0.8679\n",
            "Epoch 102/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8934 - accuracy: 0.6400 - val_loss: 0.4724 - val_accuracy: 0.8679\n",
            "Epoch 103/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9199 - accuracy: 0.6632 - val_loss: 0.4771 - val_accuracy: 0.8679\n",
            "Epoch 104/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8657 - accuracy: 0.6800 - val_loss: 0.4723 - val_accuracy: 0.8868\n",
            "Epoch 105/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8676 - accuracy: 0.6526 - val_loss: 0.4744 - val_accuracy: 0.8868\n",
            "Epoch 106/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.9457 - accuracy: 0.6316 - val_loss: 0.4762 - val_accuracy: 0.8679\n",
            "Epoch 107/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7740 - accuracy: 0.7242 - val_loss: 0.4808 - val_accuracy: 0.8679\n",
            "Epoch 108/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8998 - accuracy: 0.6505 - val_loss: 0.4698 - val_accuracy: 0.8868\n",
            "Epoch 109/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8114 - accuracy: 0.6589 - val_loss: 0.4875 - val_accuracy: 0.8679\n",
            "Epoch 110/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.9448 - accuracy: 0.6589 - val_loss: 0.4685 - val_accuracy: 0.8868\n",
            "Epoch 111/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8077 - accuracy: 0.7095 - val_loss: 0.4613 - val_accuracy: 0.9057\n",
            "Epoch 112/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8135 - accuracy: 0.7011 - val_loss: 0.4691 - val_accuracy: 0.8868\n",
            "Epoch 113/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7749 - accuracy: 0.7116 - val_loss: 0.4697 - val_accuracy: 0.8868\n",
            "Epoch 114/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8430 - accuracy: 0.6800 - val_loss: 0.4685 - val_accuracy: 0.8868\n",
            "Epoch 115/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9548 - accuracy: 0.6211 - val_loss: 0.4794 - val_accuracy: 0.8868\n",
            "Epoch 116/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8289 - accuracy: 0.6884 - val_loss: 0.4886 - val_accuracy: 0.8679\n",
            "Epoch 117/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7945 - accuracy: 0.6989 - val_loss: 0.4677 - val_accuracy: 0.9057\n",
            "Epoch 118/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7777 - accuracy: 0.7095 - val_loss: 0.4687 - val_accuracy: 0.8868\n",
            "Epoch 119/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8621 - accuracy: 0.6632 - val_loss: 0.4743 - val_accuracy: 0.8679\n",
            "Epoch 120/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8132 - accuracy: 0.6821 - val_loss: 0.4574 - val_accuracy: 0.8679\n",
            "Epoch 121/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.9114 - accuracy: 0.6295 - val_loss: 0.4576 - val_accuracy: 0.8868\n",
            "Epoch 122/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7266 - accuracy: 0.7221 - val_loss: 0.4602 - val_accuracy: 0.9057\n",
            "Epoch 123/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7967 - accuracy: 0.7032 - val_loss: 0.4541 - val_accuracy: 0.9245\n",
            "Epoch 124/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8430 - accuracy: 0.6863 - val_loss: 0.4531 - val_accuracy: 0.9057\n",
            "Epoch 125/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8584 - accuracy: 0.6758 - val_loss: 0.4432 - val_accuracy: 0.9057\n",
            "Epoch 126/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7208 - accuracy: 0.7221 - val_loss: 0.4520 - val_accuracy: 0.8868\n",
            "Epoch 127/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8501 - accuracy: 0.6758 - val_loss: 0.4535 - val_accuracy: 0.8679\n",
            "Epoch 128/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8238 - accuracy: 0.6947 - val_loss: 0.4636 - val_accuracy: 0.8868\n",
            "Epoch 129/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8215 - accuracy: 0.6842 - val_loss: 0.4422 - val_accuracy: 0.8868\n",
            "Epoch 130/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8251 - accuracy: 0.6926 - val_loss: 0.4509 - val_accuracy: 0.8868\n",
            "Epoch 131/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7734 - accuracy: 0.6968 - val_loss: 0.4637 - val_accuracy: 0.8679\n",
            "Epoch 132/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7698 - accuracy: 0.6989 - val_loss: 0.4590 - val_accuracy: 0.8868\n",
            "Epoch 133/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8328 - accuracy: 0.7074 - val_loss: 0.4379 - val_accuracy: 0.9057\n",
            "Epoch 134/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8032 - accuracy: 0.6842 - val_loss: 0.4342 - val_accuracy: 0.8868\n",
            "Epoch 135/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7822 - accuracy: 0.7032 - val_loss: 0.4332 - val_accuracy: 0.8868\n",
            "Epoch 136/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7859 - accuracy: 0.6800 - val_loss: 0.4487 - val_accuracy: 0.8868\n",
            "Epoch 137/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7741 - accuracy: 0.7095 - val_loss: 0.4565 - val_accuracy: 0.9057\n",
            "Epoch 138/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7901 - accuracy: 0.7011 - val_loss: 0.4591 - val_accuracy: 0.8868\n",
            "Epoch 139/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8549 - accuracy: 0.6674 - val_loss: 0.4566 - val_accuracy: 0.8868\n",
            "Epoch 140/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7769 - accuracy: 0.6863 - val_loss: 0.4555 - val_accuracy: 0.8868\n",
            "Epoch 141/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7969 - accuracy: 0.6989 - val_loss: 0.4427 - val_accuracy: 0.8868\n",
            "Epoch 142/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7725 - accuracy: 0.7095 - val_loss: 0.4351 - val_accuracy: 0.8868\n",
            "Epoch 143/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8002 - accuracy: 0.6884 - val_loss: 0.4325 - val_accuracy: 0.8868\n",
            "Epoch 144/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7826 - accuracy: 0.7074 - val_loss: 0.4374 - val_accuracy: 0.9057\n",
            "Epoch 145/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7832 - accuracy: 0.6947 - val_loss: 0.4341 - val_accuracy: 0.9057\n",
            "Epoch 146/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7733 - accuracy: 0.7158 - val_loss: 0.4403 - val_accuracy: 0.8868\n",
            "Epoch 147/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.7432 - val_loss: 0.4231 - val_accuracy: 0.8868\n",
            "Epoch 148/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7824 - accuracy: 0.6821 - val_loss: 0.3997 - val_accuracy: 0.9057\n",
            "Epoch 149/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7523 - accuracy: 0.7347 - val_loss: 0.4012 - val_accuracy: 0.9057\n",
            "Epoch 150/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7900 - accuracy: 0.6947 - val_loss: 0.3991 - val_accuracy: 0.9057\n",
            "Epoch 151/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7684 - accuracy: 0.7053 - val_loss: 0.4235 - val_accuracy: 0.9057\n",
            "Epoch 152/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8084 - accuracy: 0.6800 - val_loss: 0.4309 - val_accuracy: 0.8868\n",
            "Epoch 153/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8074 - accuracy: 0.7095 - val_loss: 0.4415 - val_accuracy: 0.8679\n",
            "Epoch 154/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7350 - accuracy: 0.7179 - val_loss: 0.4406 - val_accuracy: 0.8679\n",
            "Epoch 155/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7924 - accuracy: 0.7053 - val_loss: 0.4182 - val_accuracy: 0.8679\n",
            "Epoch 156/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7528 - accuracy: 0.7305 - val_loss: 0.4140 - val_accuracy: 0.9057\n",
            "Epoch 157/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8214 - accuracy: 0.6695 - val_loss: 0.4083 - val_accuracy: 0.9245\n",
            "Epoch 158/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8045 - accuracy: 0.7284 - val_loss: 0.3882 - val_accuracy: 0.9245\n",
            "Epoch 159/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8512 - accuracy: 0.6863 - val_loss: 0.3951 - val_accuracy: 0.9245\n",
            "Epoch 160/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7191 - accuracy: 0.7200 - val_loss: 0.4091 - val_accuracy: 0.9057\n",
            "Epoch 161/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7417 - accuracy: 0.7221 - val_loss: 0.4176 - val_accuracy: 0.8868\n",
            "Epoch 162/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8918 - accuracy: 0.6674 - val_loss: 0.4120 - val_accuracy: 0.8868\n",
            "Epoch 163/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7783 - accuracy: 0.6905 - val_loss: 0.4097 - val_accuracy: 0.8868\n",
            "Epoch 164/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7734 - accuracy: 0.6947 - val_loss: 0.4127 - val_accuracy: 0.8679\n",
            "Epoch 165/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7848 - accuracy: 0.7032 - val_loss: 0.4074 - val_accuracy: 0.8868\n",
            "Epoch 166/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7917 - accuracy: 0.7011 - val_loss: 0.4159 - val_accuracy: 0.9057\n",
            "Epoch 167/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8087 - accuracy: 0.7032 - val_loss: 0.4118 - val_accuracy: 0.9057\n",
            "Epoch 168/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7663 - accuracy: 0.6947 - val_loss: 0.4232 - val_accuracy: 0.8868\n",
            "Epoch 169/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7264 - accuracy: 0.7179 - val_loss: 0.4212 - val_accuracy: 0.8868\n",
            "Epoch 170/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7934 - accuracy: 0.7074 - val_loss: 0.4171 - val_accuracy: 0.8868\n",
            "Epoch 171/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8139 - accuracy: 0.6758 - val_loss: 0.4028 - val_accuracy: 0.8868\n",
            "Epoch 172/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7500 - accuracy: 0.7200 - val_loss: 0.3940 - val_accuracy: 0.8868\n",
            "Epoch 173/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7547 - accuracy: 0.7158 - val_loss: 0.3824 - val_accuracy: 0.9245\n",
            "Epoch 174/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7811 - accuracy: 0.7053 - val_loss: 0.3725 - val_accuracy: 0.9245\n",
            "Epoch 175/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8110 - accuracy: 0.7221 - val_loss: 0.3782 - val_accuracy: 0.9057\n",
            "Epoch 176/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7302 - accuracy: 0.7368 - val_loss: 0.3855 - val_accuracy: 0.9057\n",
            "Epoch 177/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6556 - accuracy: 0.7411 - val_loss: 0.3882 - val_accuracy: 0.9245\n",
            "Epoch 178/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8494 - accuracy: 0.6800 - val_loss: 0.3671 - val_accuracy: 0.9245\n",
            "Epoch 179/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8285 - accuracy: 0.6800 - val_loss: 0.3662 - val_accuracy: 0.9245\n",
            "Epoch 180/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.8247 - accuracy: 0.6884 - val_loss: 0.3904 - val_accuracy: 0.8868\n",
            "Epoch 181/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7167 - accuracy: 0.7137 - val_loss: 0.4023 - val_accuracy: 0.8679\n",
            "Epoch 182/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7825 - accuracy: 0.6926 - val_loss: 0.4042 - val_accuracy: 0.9057\n",
            "Epoch 183/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7318 - accuracy: 0.7326 - val_loss: 0.3993 - val_accuracy: 0.8868\n",
            "Epoch 184/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7542 - accuracy: 0.7284 - val_loss: 0.4059 - val_accuracy: 0.8868\n",
            "Epoch 185/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7909 - accuracy: 0.6905 - val_loss: 0.3899 - val_accuracy: 0.8868\n",
            "Epoch 186/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7763 - accuracy: 0.7263 - val_loss: 0.3923 - val_accuracy: 0.8868\n",
            "Epoch 187/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7241 - accuracy: 0.7200 - val_loss: 0.3925 - val_accuracy: 0.8868\n",
            "Epoch 188/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7561 - accuracy: 0.6863 - val_loss: 0.3857 - val_accuracy: 0.8679\n",
            "Epoch 189/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8522 - accuracy: 0.6947 - val_loss: 0.4021 - val_accuracy: 0.8679\n",
            "Epoch 190/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7283 - accuracy: 0.6863 - val_loss: 0.4089 - val_accuracy: 0.8679\n",
            "Epoch 191/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7306 - accuracy: 0.7137 - val_loss: 0.4054 - val_accuracy: 0.8679\n",
            "Epoch 192/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7367 - accuracy: 0.7158 - val_loss: 0.3843 - val_accuracy: 0.8868\n",
            "Epoch 193/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8213 - accuracy: 0.6842 - val_loss: 0.4068 - val_accuracy: 0.8868\n",
            "Epoch 194/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7191 - accuracy: 0.7116 - val_loss: 0.3971 - val_accuracy: 0.8868\n",
            "Epoch 195/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7815 - accuracy: 0.7032 - val_loss: 0.3873 - val_accuracy: 0.8868\n",
            "Epoch 196/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8322 - accuracy: 0.7053 - val_loss: 0.4053 - val_accuracy: 0.8679\n",
            "Epoch 197/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7446 - accuracy: 0.7474 - val_loss: 0.4066 - val_accuracy: 0.8679\n",
            "Epoch 198/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7962 - accuracy: 0.7053 - val_loss: 0.3788 - val_accuracy: 0.9245\n",
            "Epoch 199/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7740 - accuracy: 0.7053 - val_loss: 0.3807 - val_accuracy: 0.8679\n",
            "Epoch 200/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7449 - accuracy: 0.7158 - val_loss: 0.3765 - val_accuracy: 0.9057\n",
            "Epoch 201/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7221 - accuracy: 0.7242 - val_loss: 0.3838 - val_accuracy: 0.8868\n",
            "Epoch 202/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8153 - accuracy: 0.6926 - val_loss: 0.3956 - val_accuracy: 0.8679\n",
            "Epoch 203/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7408 - accuracy: 0.7179 - val_loss: 0.3895 - val_accuracy: 0.9245\n",
            "Epoch 204/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7421 - accuracy: 0.7116 - val_loss: 0.3706 - val_accuracy: 0.9245\n",
            "Epoch 205/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7822 - accuracy: 0.7032 - val_loss: 0.3562 - val_accuracy: 0.9434\n",
            "Epoch 206/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7763 - accuracy: 0.7074 - val_loss: 0.3623 - val_accuracy: 0.9245\n",
            "Epoch 207/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6977 - accuracy: 0.7495 - val_loss: 0.3613 - val_accuracy: 0.9245\n",
            "Epoch 208/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8210 - accuracy: 0.7200 - val_loss: 0.3668 - val_accuracy: 0.8868\n",
            "Epoch 209/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8688 - accuracy: 0.6716 - val_loss: 0.3668 - val_accuracy: 0.8868\n",
            "Epoch 210/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7078 - accuracy: 0.7368 - val_loss: 0.3584 - val_accuracy: 0.9245\n",
            "Epoch 211/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6973 - accuracy: 0.7284 - val_loss: 0.3518 - val_accuracy: 0.9245\n",
            "Epoch 212/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.7600 - val_loss: 0.3613 - val_accuracy: 0.9057\n",
            "Epoch 213/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6474 - accuracy: 0.7389 - val_loss: 0.3745 - val_accuracy: 0.9057\n",
            "Epoch 214/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7840 - accuracy: 0.7179 - val_loss: 0.3823 - val_accuracy: 0.9057\n",
            "Epoch 215/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7471 - accuracy: 0.7137 - val_loss: 0.3772 - val_accuracy: 0.9245\n",
            "Epoch 216/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7170 - accuracy: 0.7389 - val_loss: 0.3666 - val_accuracy: 0.9057\n",
            "Epoch 217/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7269 - accuracy: 0.7579 - val_loss: 0.3803 - val_accuracy: 0.9057\n",
            "Epoch 218/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8015 - accuracy: 0.7305 - val_loss: 0.3740 - val_accuracy: 0.9434\n",
            "Epoch 219/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6943 - accuracy: 0.7389 - val_loss: 0.3667 - val_accuracy: 0.9434\n",
            "Epoch 220/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7574 - accuracy: 0.7179 - val_loss: 0.3754 - val_accuracy: 0.9245\n",
            "Epoch 221/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7458 - accuracy: 0.6884 - val_loss: 0.3562 - val_accuracy: 0.9245\n",
            "Epoch 222/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7309 - accuracy: 0.7095 - val_loss: 0.3600 - val_accuracy: 0.9057\n",
            "Epoch 223/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7093 - accuracy: 0.7411 - val_loss: 0.3590 - val_accuracy: 0.9245\n",
            "Epoch 224/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7426 - accuracy: 0.7116 - val_loss: 0.3378 - val_accuracy: 0.9434\n",
            "Epoch 225/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7428 - accuracy: 0.7116 - val_loss: 0.3451 - val_accuracy: 0.9245\n",
            "Epoch 226/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7621 - accuracy: 0.7137 - val_loss: 0.3533 - val_accuracy: 0.9245\n",
            "Epoch 227/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7669 - accuracy: 0.7263 - val_loss: 0.3706 - val_accuracy: 0.8868\n",
            "Epoch 228/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.7368 - val_loss: 0.3776 - val_accuracy: 0.8679\n",
            "Epoch 229/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6892 - accuracy: 0.7516 - val_loss: 0.3802 - val_accuracy: 0.8679\n",
            "Epoch 230/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7282 - accuracy: 0.7242 - val_loss: 0.3728 - val_accuracy: 0.9057\n",
            "Epoch 231/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7304 - accuracy: 0.7011 - val_loss: 0.3605 - val_accuracy: 0.9245\n",
            "Epoch 232/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7503 - accuracy: 0.7347 - val_loss: 0.3537 - val_accuracy: 0.9057\n",
            "Epoch 233/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7796 - accuracy: 0.6800 - val_loss: 0.3481 - val_accuracy: 0.9057\n",
            "Epoch 234/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7049 - accuracy: 0.7242 - val_loss: 0.3453 - val_accuracy: 0.9245\n",
            "Epoch 235/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7270 - accuracy: 0.7284 - val_loss: 0.3492 - val_accuracy: 0.9245\n",
            "Epoch 236/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7581 - accuracy: 0.6989 - val_loss: 0.3503 - val_accuracy: 0.9245\n",
            "Epoch 237/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7762 - accuracy: 0.7137 - val_loss: 0.3583 - val_accuracy: 0.9245\n",
            "Epoch 238/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7533 - accuracy: 0.7137 - val_loss: 0.3629 - val_accuracy: 0.9245\n",
            "Epoch 239/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7438 - accuracy: 0.7389 - val_loss: 0.3524 - val_accuracy: 0.9245\n",
            "Epoch 240/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7748 - accuracy: 0.7116 - val_loss: 0.3697 - val_accuracy: 0.9245\n",
            "Epoch 241/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.7537 - val_loss: 0.3626 - val_accuracy: 0.9057\n",
            "Epoch 242/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6577 - accuracy: 0.7537 - val_loss: 0.3503 - val_accuracy: 0.9245\n",
            "Epoch 243/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6805 - accuracy: 0.7389 - val_loss: 0.3582 - val_accuracy: 0.9057\n",
            "Epoch 244/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7535 - accuracy: 0.7284 - val_loss: 0.3547 - val_accuracy: 0.9057\n",
            "Epoch 245/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6574 - accuracy: 0.7516 - val_loss: 0.3421 - val_accuracy: 0.9057\n",
            "Epoch 246/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7084 - accuracy: 0.7579 - val_loss: 0.3546 - val_accuracy: 0.8868\n",
            "Epoch 247/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7962 - accuracy: 0.6632 - val_loss: 0.3541 - val_accuracy: 0.9245\n",
            "Epoch 248/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.7200 - val_loss: 0.3486 - val_accuracy: 0.9057\n",
            "Epoch 249/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6988 - accuracy: 0.7389 - val_loss: 0.3654 - val_accuracy: 0.9057\n",
            "Epoch 250/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.7242 - val_loss: 0.3795 - val_accuracy: 0.8868\n",
            "Epoch 251/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7518 - accuracy: 0.7263 - val_loss: 0.3867 - val_accuracy: 0.9057\n",
            "Epoch 252/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7158 - accuracy: 0.7495 - val_loss: 0.3632 - val_accuracy: 0.9057\n",
            "Epoch 253/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7700 - accuracy: 0.7200 - val_loss: 0.3589 - val_accuracy: 0.9245\n",
            "Epoch 254/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7308 - accuracy: 0.7263 - val_loss: 0.3638 - val_accuracy: 0.8868\n",
            "Epoch 255/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7784 - accuracy: 0.7032 - val_loss: 0.3567 - val_accuracy: 0.8868\n",
            "Epoch 256/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7472 - accuracy: 0.7368 - val_loss: 0.3640 - val_accuracy: 0.9245\n",
            "Epoch 257/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.7537 - val_loss: 0.3540 - val_accuracy: 0.9245\n",
            "Epoch 258/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8117 - accuracy: 0.6947 - val_loss: 0.3524 - val_accuracy: 0.9245\n",
            "Epoch 259/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6950 - accuracy: 0.7326 - val_loss: 0.3479 - val_accuracy: 0.9434\n",
            "Epoch 260/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8031 - accuracy: 0.6989 - val_loss: 0.3452 - val_accuracy: 0.9245\n",
            "Epoch 261/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6270 - accuracy: 0.7811 - val_loss: 0.3328 - val_accuracy: 0.9434\n",
            "Epoch 262/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6035 - accuracy: 0.7747 - val_loss: 0.3248 - val_accuracy: 0.9434\n",
            "Epoch 263/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7736 - accuracy: 0.7158 - val_loss: 0.3336 - val_accuracy: 0.9434\n",
            "Epoch 264/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7265 - accuracy: 0.7368 - val_loss: 0.3375 - val_accuracy: 0.9434\n",
            "Epoch 265/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6913 - accuracy: 0.7263 - val_loss: 0.3437 - val_accuracy: 0.9245\n",
            "Epoch 266/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7337 - accuracy: 0.7389 - val_loss: 0.3544 - val_accuracy: 0.9245\n",
            "Epoch 267/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7769 - accuracy: 0.7179 - val_loss: 0.3485 - val_accuracy: 0.9245\n",
            "Epoch 268/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8058 - accuracy: 0.7095 - val_loss: 0.3793 - val_accuracy: 0.8868\n",
            "Epoch 269/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6337 - accuracy: 0.7789 - val_loss: 0.3768 - val_accuracy: 0.8868\n",
            "Epoch 270/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7409 - accuracy: 0.7368 - val_loss: 0.3676 - val_accuracy: 0.8868\n",
            "Epoch 271/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6805 - accuracy: 0.7389 - val_loss: 0.3564 - val_accuracy: 0.9245\n",
            "Epoch 272/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6666 - accuracy: 0.7284 - val_loss: 0.3553 - val_accuracy: 0.9057\n",
            "Epoch 273/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6412 - accuracy: 0.7642 - val_loss: 0.3468 - val_accuracy: 0.9245\n",
            "Epoch 274/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7176 - accuracy: 0.7368 - val_loss: 0.3472 - val_accuracy: 0.9245\n",
            "Epoch 275/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7982 - accuracy: 0.6947 - val_loss: 0.3391 - val_accuracy: 0.9245\n",
            "Epoch 276/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.7726 - val_loss: 0.3268 - val_accuracy: 0.9434\n",
            "Epoch 277/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7005 - accuracy: 0.7495 - val_loss: 0.3289 - val_accuracy: 0.9245\n",
            "Epoch 278/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6754 - accuracy: 0.7411 - val_loss: 0.3181 - val_accuracy: 0.9245\n",
            "Epoch 279/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7435 - accuracy: 0.7284 - val_loss: 0.3215 - val_accuracy: 0.9434\n",
            "Epoch 280/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7432 - accuracy: 0.7137 - val_loss: 0.3396 - val_accuracy: 0.9245\n",
            "Epoch 281/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7700 - accuracy: 0.7053 - val_loss: 0.3381 - val_accuracy: 0.9245\n",
            "Epoch 282/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7735 - accuracy: 0.7305 - val_loss: 0.3482 - val_accuracy: 0.9434\n",
            "Epoch 283/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7092 - accuracy: 0.7053 - val_loss: 0.3501 - val_accuracy: 0.9245\n",
            "Epoch 284/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7451 - accuracy: 0.7179 - val_loss: 0.3580 - val_accuracy: 0.9057\n",
            "Epoch 285/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7343 - accuracy: 0.7200 - val_loss: 0.3719 - val_accuracy: 0.8868\n",
            "Epoch 286/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7854 - accuracy: 0.7326 - val_loss: 0.3589 - val_accuracy: 0.9245\n",
            "Epoch 287/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7042 - accuracy: 0.7137 - val_loss: 0.3556 - val_accuracy: 0.9245\n",
            "Epoch 288/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.7913 - accuracy: 0.7116 - val_loss: 0.3616 - val_accuracy: 0.9434\n",
            "Epoch 289/500\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.7347 - accuracy: 0.7326 - val_loss: 0.3617 - val_accuracy: 0.9245\n",
            "Epoch 290/500\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.7313 - accuracy: 0.7516 - val_loss: 0.3546 - val_accuracy: 0.9434\n",
            "Epoch 291/500\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.7808 - accuracy: 0.7137 - val_loss: 0.3451 - val_accuracy: 0.9245\n",
            "Epoch 292/500\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.7234 - accuracy: 0.7305 - val_loss: 0.3469 - val_accuracy: 0.9245\n",
            "Epoch 293/500\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 0.8025 - accuracy: 0.7137 - val_loss: 0.3575 - val_accuracy: 0.9434\n",
            "Epoch 294/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.6540 - accuracy: 0.7389 - val_loss: 0.3636 - val_accuracy: 0.9057\n",
            "Epoch 295/500\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.7100 - accuracy: 0.7432 - val_loss: 0.3469 - val_accuracy: 0.9245\n",
            "Epoch 296/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.6285 - accuracy: 0.7663 - val_loss: 0.3339 - val_accuracy: 0.9245\n",
            "Epoch 297/500\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.6919 - accuracy: 0.7537 - val_loss: 0.3293 - val_accuracy: 0.9057\n",
            "Epoch 298/500\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 0.6160 - accuracy: 0.7663 - val_loss: 0.3321 - val_accuracy: 0.9434\n",
            "Epoch 299/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.6966 - accuracy: 0.7621 - val_loss: 0.3440 - val_accuracy: 0.9434\n",
            "Epoch 300/500\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6805 - accuracy: 0.7495 - val_loss: 0.3411 - val_accuracy: 0.9245\n",
            "Epoch 301/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7087 - accuracy: 0.7516 - val_loss: 0.3382 - val_accuracy: 0.9245\n",
            "Epoch 302/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7163 - accuracy: 0.7284 - val_loss: 0.3513 - val_accuracy: 0.9245\n",
            "Epoch 303/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7106 - accuracy: 0.7495 - val_loss: 0.3613 - val_accuracy: 0.9057\n",
            "Epoch 304/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7156 - accuracy: 0.7432 - val_loss: 0.3577 - val_accuracy: 0.9057\n",
            "Epoch 305/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7042 - accuracy: 0.7453 - val_loss: 0.3439 - val_accuracy: 0.9057\n",
            "Epoch 306/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7575 - accuracy: 0.7074 - val_loss: 0.3368 - val_accuracy: 0.9245\n",
            "Epoch 307/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.6739 - accuracy: 0.7663 - val_loss: 0.3251 - val_accuracy: 0.9245\n",
            "Epoch 308/500\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.7675 - accuracy: 0.7158 - val_loss: 0.3294 - val_accuracy: 0.9245\n",
            "Epoch 309/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7779 - accuracy: 0.7074 - val_loss: 0.3225 - val_accuracy: 0.9245\n",
            "Epoch 310/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.6890 - accuracy: 0.7284 - val_loss: 0.3210 - val_accuracy: 0.9434\n",
            "Epoch 311/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.7172 - accuracy: 0.7453 - val_loss: 0.3289 - val_accuracy: 0.9434\n",
            "Epoch 312/500\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 0.6939 - accuracy: 0.7432 - val_loss: 0.3323 - val_accuracy: 0.9245\n",
            "Epoch 313/500\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.7184 - accuracy: 0.7284 - val_loss: 0.3415 - val_accuracy: 0.9434\n",
            "Epoch 314/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.8193 - accuracy: 0.6842 - val_loss: 0.3203 - val_accuracy: 0.9057\n",
            "Epoch 315/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.6651 - accuracy: 0.7558 - val_loss: 0.3281 - val_accuracy: 0.9057\n",
            "Epoch 316/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6696 - accuracy: 0.7642 - val_loss: 0.3298 - val_accuracy: 0.9245\n",
            "Epoch 317/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7529 - accuracy: 0.7242 - val_loss: 0.3466 - val_accuracy: 0.9057\n",
            "Epoch 318/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.6818 - accuracy: 0.7600 - val_loss: 0.3432 - val_accuracy: 0.9057\n",
            "Epoch 319/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6211 - accuracy: 0.7579 - val_loss: 0.3128 - val_accuracy: 0.9434\n",
            "Epoch 320/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7101 - accuracy: 0.7305 - val_loss: 0.3168 - val_accuracy: 0.9434\n",
            "Epoch 321/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6732 - accuracy: 0.7853 - val_loss: 0.3228 - val_accuracy: 0.9245\n",
            "Epoch 322/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6764 - accuracy: 0.7221 - val_loss: 0.3156 - val_accuracy: 0.9057\n",
            "Epoch 323/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6722 - accuracy: 0.7537 - val_loss: 0.3252 - val_accuracy: 0.9057\n",
            "Epoch 324/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7074 - accuracy: 0.7137 - val_loss: 0.3268 - val_accuracy: 0.9245\n",
            "Epoch 325/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6595 - accuracy: 0.7621 - val_loss: 0.3260 - val_accuracy: 0.9245\n",
            "Epoch 326/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.6023 - accuracy: 0.7958 - val_loss: 0.3190 - val_accuracy: 0.9245\n",
            "Epoch 327/500\n",
            "30/30 [==============================] - 0s 6ms/step - loss: 0.6857 - accuracy: 0.7558 - val_loss: 0.3187 - val_accuracy: 0.9434\n",
            "Epoch 328/500\n",
            "30/30 [==============================] - 0s 7ms/step - loss: 0.6884 - accuracy: 0.7389 - val_loss: 0.3096 - val_accuracy: 0.9434\n",
            "Epoch 329/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6783 - accuracy: 0.7474 - val_loss: 0.3181 - val_accuracy: 0.9245\n",
            "Epoch 330/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7015 - accuracy: 0.7263 - val_loss: 0.3304 - val_accuracy: 0.9245\n",
            "Epoch 331/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8429 - accuracy: 0.6884 - val_loss: 0.3357 - val_accuracy: 0.8868\n",
            "Epoch 332/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7916 - accuracy: 0.7074 - val_loss: 0.3414 - val_accuracy: 0.9057\n",
            "Epoch 333/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7868 - accuracy: 0.7326 - val_loss: 0.3467 - val_accuracy: 0.8868\n",
            "Epoch 334/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7210 - accuracy: 0.6884 - val_loss: 0.3536 - val_accuracy: 0.9057\n",
            "Epoch 335/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6573 - accuracy: 0.7495 - val_loss: 0.3491 - val_accuracy: 0.9057\n",
            "Epoch 336/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6739 - accuracy: 0.7453 - val_loss: 0.3472 - val_accuracy: 0.9057\n",
            "Epoch 337/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7109 - accuracy: 0.7242 - val_loss: 0.3350 - val_accuracy: 0.9245\n",
            "Epoch 338/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6780 - accuracy: 0.7474 - val_loss: 0.3430 - val_accuracy: 0.9057\n",
            "Epoch 339/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7703 - accuracy: 0.7242 - val_loss: 0.3500 - val_accuracy: 0.9057\n",
            "Epoch 340/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8106 - accuracy: 0.7011 - val_loss: 0.3563 - val_accuracy: 0.9057\n",
            "Epoch 341/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7602 - accuracy: 0.7284 - val_loss: 0.3413 - val_accuracy: 0.9057\n",
            "Epoch 342/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6497 - accuracy: 0.7600 - val_loss: 0.3277 - val_accuracy: 0.8868\n",
            "Epoch 343/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7489 - accuracy: 0.7221 - val_loss: 0.3380 - val_accuracy: 0.9057\n",
            "Epoch 344/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6195 - accuracy: 0.7726 - val_loss: 0.3281 - val_accuracy: 0.9057\n",
            "Epoch 345/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6223 - accuracy: 0.7579 - val_loss: 0.3269 - val_accuracy: 0.9057\n",
            "Epoch 346/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7637 - accuracy: 0.7053 - val_loss: 0.3237 - val_accuracy: 0.9057\n",
            "Epoch 347/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7267 - accuracy: 0.7263 - val_loss: 0.3271 - val_accuracy: 0.8868\n",
            "Epoch 348/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6638 - accuracy: 0.7684 - val_loss: 0.3424 - val_accuracy: 0.8868\n",
            "Epoch 349/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7081 - accuracy: 0.7495 - val_loss: 0.3370 - val_accuracy: 0.8868\n",
            "Epoch 350/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6964 - accuracy: 0.7600 - val_loss: 0.3347 - val_accuracy: 0.8868\n",
            "Epoch 351/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.7347 - val_loss: 0.3374 - val_accuracy: 0.9245\n",
            "Epoch 352/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.7600 - val_loss: 0.3438 - val_accuracy: 0.9057\n",
            "Epoch 353/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5591 - accuracy: 0.8042 - val_loss: 0.3430 - val_accuracy: 0.9057\n",
            "Epoch 354/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8195 - accuracy: 0.6926 - val_loss: 0.3314 - val_accuracy: 0.9057\n",
            "Epoch 355/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7476 - accuracy: 0.7221 - val_loss: 0.3365 - val_accuracy: 0.9245\n",
            "Epoch 356/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7043 - accuracy: 0.7411 - val_loss: 0.3292 - val_accuracy: 0.9057\n",
            "Epoch 357/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.7368 - val_loss: 0.3268 - val_accuracy: 0.8868\n",
            "Epoch 358/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7141 - accuracy: 0.7453 - val_loss: 0.3318 - val_accuracy: 0.8679\n",
            "Epoch 359/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5952 - accuracy: 0.7937 - val_loss: 0.3214 - val_accuracy: 0.8868\n",
            "Epoch 360/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7023 - accuracy: 0.7347 - val_loss: 0.3228 - val_accuracy: 0.8868\n",
            "Epoch 361/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6720 - accuracy: 0.7474 - val_loss: 0.3102 - val_accuracy: 0.9057\n",
            "Epoch 362/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6814 - accuracy: 0.7558 - val_loss: 0.3108 - val_accuracy: 0.9245\n",
            "Epoch 363/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6424 - accuracy: 0.7747 - val_loss: 0.3036 - val_accuracy: 0.9245\n",
            "Epoch 364/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.7453 - val_loss: 0.3031 - val_accuracy: 0.9434\n",
            "Epoch 365/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6077 - accuracy: 0.7789 - val_loss: 0.3068 - val_accuracy: 0.8868\n",
            "Epoch 366/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6660 - accuracy: 0.7621 - val_loss: 0.3218 - val_accuracy: 0.8868\n",
            "Epoch 367/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6419 - accuracy: 0.7726 - val_loss: 0.3314 - val_accuracy: 0.8679\n",
            "Epoch 368/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.7558 - val_loss: 0.3152 - val_accuracy: 0.8679\n",
            "Epoch 369/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7357 - accuracy: 0.7137 - val_loss: 0.3035 - val_accuracy: 0.8868\n",
            "Epoch 370/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5813 - accuracy: 0.7979 - val_loss: 0.3152 - val_accuracy: 0.8868\n",
            "Epoch 371/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5834 - accuracy: 0.7853 - val_loss: 0.3190 - val_accuracy: 0.8679\n",
            "Epoch 372/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6106 - accuracy: 0.7726 - val_loss: 0.3210 - val_accuracy: 0.8679\n",
            "Epoch 373/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6383 - accuracy: 0.7832 - val_loss: 0.3247 - val_accuracy: 0.8868\n",
            "Epoch 374/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.7705 - val_loss: 0.3325 - val_accuracy: 0.8868\n",
            "Epoch 375/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.7537 - val_loss: 0.3497 - val_accuracy: 0.8302\n",
            "Epoch 376/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.7558 - val_loss: 0.3448 - val_accuracy: 0.8302\n",
            "Epoch 377/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6699 - accuracy: 0.7389 - val_loss: 0.3273 - val_accuracy: 0.8491\n",
            "Epoch 378/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.7621 - val_loss: 0.3349 - val_accuracy: 0.8302\n",
            "Epoch 379/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6780 - accuracy: 0.7516 - val_loss: 0.3228 - val_accuracy: 0.8868\n",
            "Epoch 380/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8213 - accuracy: 0.6989 - val_loss: 0.3320 - val_accuracy: 0.8868\n",
            "Epoch 381/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6151 - accuracy: 0.7853 - val_loss: 0.3322 - val_accuracy: 0.8868\n",
            "Epoch 382/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6160 - accuracy: 0.7621 - val_loss: 0.3235 - val_accuracy: 0.8868\n",
            "Epoch 383/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7416 - accuracy: 0.7200 - val_loss: 0.3168 - val_accuracy: 0.8868\n",
            "Epoch 384/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.7275 - accuracy: 0.7053 - val_loss: 0.3177 - val_accuracy: 0.8868\n",
            "Epoch 385/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7580 - accuracy: 0.7516 - val_loss: 0.3202 - val_accuracy: 0.9057\n",
            "Epoch 386/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.7684 - val_loss: 0.3152 - val_accuracy: 0.9245\n",
            "Epoch 387/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6044 - accuracy: 0.7768 - val_loss: 0.3106 - val_accuracy: 0.9057\n",
            "Epoch 388/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6250 - accuracy: 0.7811 - val_loss: 0.3215 - val_accuracy: 0.9057\n",
            "Epoch 389/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.7474 - val_loss: 0.3067 - val_accuracy: 0.9057\n",
            "Epoch 390/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6204 - accuracy: 0.7705 - val_loss: 0.3182 - val_accuracy: 0.9057\n",
            "Epoch 391/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6991 - accuracy: 0.7326 - val_loss: 0.3103 - val_accuracy: 0.9434\n",
            "Epoch 392/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6974 - accuracy: 0.7579 - val_loss: 0.3164 - val_accuracy: 0.8868\n",
            "Epoch 393/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.7149 - accuracy: 0.7284 - val_loss: 0.3161 - val_accuracy: 0.8679\n",
            "Epoch 394/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6351 - accuracy: 0.7558 - val_loss: 0.3267 - val_accuracy: 0.8868\n",
            "Epoch 395/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6190 - accuracy: 0.7495 - val_loss: 0.3194 - val_accuracy: 0.9057\n",
            "Epoch 396/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7624 - accuracy: 0.7221 - val_loss: 0.3134 - val_accuracy: 0.9434\n",
            "Epoch 397/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6300 - accuracy: 0.7789 - val_loss: 0.3070 - val_accuracy: 0.8868\n",
            "Epoch 398/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6491 - accuracy: 0.7663 - val_loss: 0.3155 - val_accuracy: 0.8868\n",
            "Epoch 399/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6704 - accuracy: 0.7621 - val_loss: 0.3047 - val_accuracy: 0.9245\n",
            "Epoch 400/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8299 - accuracy: 0.6884 - val_loss: 0.3025 - val_accuracy: 0.9245\n",
            "Epoch 401/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7539 - accuracy: 0.7411 - val_loss: 0.3095 - val_accuracy: 0.9245\n",
            "Epoch 402/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6213 - accuracy: 0.7958 - val_loss: 0.3056 - val_accuracy: 0.9245\n",
            "Epoch 403/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.7516 - val_loss: 0.3209 - val_accuracy: 0.9057\n",
            "Epoch 404/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5988 - accuracy: 0.7895 - val_loss: 0.3076 - val_accuracy: 0.9057\n",
            "Epoch 405/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7842 - accuracy: 0.6968 - val_loss: 0.3080 - val_accuracy: 0.9057\n",
            "Epoch 406/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.7305 - val_loss: 0.3182 - val_accuracy: 0.9057\n",
            "Epoch 407/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7627 - accuracy: 0.7200 - val_loss: 0.3246 - val_accuracy: 0.8868\n",
            "Epoch 408/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7466 - accuracy: 0.7284 - val_loss: 0.3323 - val_accuracy: 0.8679\n",
            "Epoch 409/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7546 - accuracy: 0.7137 - val_loss: 0.3224 - val_accuracy: 0.8679\n",
            "Epoch 410/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.7347 - val_loss: 0.3150 - val_accuracy: 0.9057\n",
            "Epoch 411/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6360 - accuracy: 0.7642 - val_loss: 0.3141 - val_accuracy: 0.9245\n",
            "Epoch 412/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7056 - accuracy: 0.7200 - val_loss: 0.3246 - val_accuracy: 0.8679\n",
            "Epoch 413/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6423 - accuracy: 0.7726 - val_loss: 0.3256 - val_accuracy: 0.8679\n",
            "Epoch 414/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7485 - accuracy: 0.7326 - val_loss: 0.3380 - val_accuracy: 0.8679\n",
            "Epoch 415/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6685 - accuracy: 0.7579 - val_loss: 0.3330 - val_accuracy: 0.8491\n",
            "Epoch 416/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7098 - accuracy: 0.7411 - val_loss: 0.3265 - val_accuracy: 0.8868\n",
            "Epoch 417/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7394 - accuracy: 0.7453 - val_loss: 0.3259 - val_accuracy: 0.8868\n",
            "Epoch 418/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7143 - accuracy: 0.7305 - val_loss: 0.3205 - val_accuracy: 0.8868\n",
            "Epoch 419/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6785 - accuracy: 0.7347 - val_loss: 0.3191 - val_accuracy: 0.8868\n",
            "Epoch 420/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.7242 - val_loss: 0.3283 - val_accuracy: 0.8868\n",
            "Epoch 421/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6412 - accuracy: 0.7537 - val_loss: 0.3106 - val_accuracy: 0.9057\n",
            "Epoch 422/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7212 - accuracy: 0.7432 - val_loss: 0.3097 - val_accuracy: 0.9057\n",
            "Epoch 423/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7196 - accuracy: 0.7221 - val_loss: 0.3116 - val_accuracy: 0.9057\n",
            "Epoch 424/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7736 - accuracy: 0.7116 - val_loss: 0.3293 - val_accuracy: 0.8868\n",
            "Epoch 425/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.7747 - val_loss: 0.3275 - val_accuracy: 0.9057\n",
            "Epoch 426/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.7389 - val_loss: 0.3228 - val_accuracy: 0.9057\n",
            "Epoch 427/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6131 - accuracy: 0.7811 - val_loss: 0.3169 - val_accuracy: 0.9057\n",
            "Epoch 428/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.7516 - val_loss: 0.3288 - val_accuracy: 0.9057\n",
            "Epoch 429/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5777 - accuracy: 0.7979 - val_loss: 0.3262 - val_accuracy: 0.8679\n",
            "Epoch 430/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5695 - accuracy: 0.8063 - val_loss: 0.3053 - val_accuracy: 0.8868\n",
            "Epoch 431/500\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 0.6280 - accuracy: 0.7705 - val_loss: 0.3007 - val_accuracy: 0.9245\n",
            "Epoch 432/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7239 - accuracy: 0.7263 - val_loss: 0.3174 - val_accuracy: 0.8679\n",
            "Epoch 433/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6436 - accuracy: 0.7495 - val_loss: 0.3158 - val_accuracy: 0.9057\n",
            "Epoch 434/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.7684 - val_loss: 0.3138 - val_accuracy: 0.8868\n",
            "Epoch 435/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6315 - accuracy: 0.7684 - val_loss: 0.2927 - val_accuracy: 0.9434\n",
            "Epoch 436/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6416 - accuracy: 0.7684 - val_loss: 0.2967 - val_accuracy: 0.9057\n",
            "Epoch 437/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7067 - accuracy: 0.7284 - val_loss: 0.3016 - val_accuracy: 0.9434\n",
            "Epoch 438/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6123 - accuracy: 0.7747 - val_loss: 0.2984 - val_accuracy: 0.9623\n",
            "Epoch 439/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6134 - accuracy: 0.7832 - val_loss: 0.2997 - val_accuracy: 0.9245\n",
            "Epoch 440/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6432 - accuracy: 0.7621 - val_loss: 0.2918 - val_accuracy: 0.9245\n",
            "Epoch 441/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7153 - accuracy: 0.7453 - val_loss: 0.3084 - val_accuracy: 0.8868\n",
            "Epoch 442/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6294 - accuracy: 0.7705 - val_loss: 0.2943 - val_accuracy: 0.9245\n",
            "Epoch 443/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.7621 - val_loss: 0.2975 - val_accuracy: 0.9245\n",
            "Epoch 444/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7053 - accuracy: 0.7516 - val_loss: 0.3108 - val_accuracy: 0.8679\n",
            "Epoch 445/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.7979 - val_loss: 0.3006 - val_accuracy: 0.8868\n",
            "Epoch 446/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6688 - accuracy: 0.7368 - val_loss: 0.2968 - val_accuracy: 0.9245\n",
            "Epoch 447/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.7305 - val_loss: 0.3034 - val_accuracy: 0.9057\n",
            "Epoch 448/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.8035 - accuracy: 0.7011 - val_loss: 0.3163 - val_accuracy: 0.8868\n",
            "Epoch 449/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5875 - accuracy: 0.7663 - val_loss: 0.3022 - val_accuracy: 0.9057\n",
            "Epoch 450/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6235 - accuracy: 0.7726 - val_loss: 0.3024 - val_accuracy: 0.9434\n",
            "Epoch 451/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6718 - accuracy: 0.7495 - val_loss: 0.3017 - val_accuracy: 0.9057\n",
            "Epoch 452/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7692 - accuracy: 0.7368 - val_loss: 0.3037 - val_accuracy: 0.9057\n",
            "Epoch 453/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6449 - accuracy: 0.7811 - val_loss: 0.3095 - val_accuracy: 0.9057\n",
            "Epoch 454/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7931 - accuracy: 0.6926 - val_loss: 0.3224 - val_accuracy: 0.8868\n",
            "Epoch 455/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5837 - accuracy: 0.7853 - val_loss: 0.3066 - val_accuracy: 0.8868\n",
            "Epoch 456/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.7432 - val_loss: 0.3011 - val_accuracy: 0.9245\n",
            "Epoch 457/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6885 - accuracy: 0.7537 - val_loss: 0.2955 - val_accuracy: 0.9434\n",
            "Epoch 458/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6236 - accuracy: 0.7663 - val_loss: 0.2989 - val_accuracy: 0.9057\n",
            "Epoch 459/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7573 - accuracy: 0.7074 - val_loss: 0.2989 - val_accuracy: 0.8868\n",
            "Epoch 460/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5976 - accuracy: 0.7747 - val_loss: 0.3048 - val_accuracy: 0.8679\n",
            "Epoch 461/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7227 - accuracy: 0.7432 - val_loss: 0.2956 - val_accuracy: 0.9245\n",
            "Epoch 462/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6246 - accuracy: 0.7811 - val_loss: 0.2890 - val_accuracy: 0.9245\n",
            "Epoch 463/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.7411 - val_loss: 0.2944 - val_accuracy: 0.9434\n",
            "Epoch 464/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5922 - accuracy: 0.7642 - val_loss: 0.2916 - val_accuracy: 0.9434\n",
            "Epoch 465/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5562 - accuracy: 0.7916 - val_loss: 0.3010 - val_accuracy: 0.9434\n",
            "Epoch 466/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.7453 - val_loss: 0.2995 - val_accuracy: 0.9245\n",
            "Epoch 467/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7784 - accuracy: 0.7242 - val_loss: 0.3140 - val_accuracy: 0.9057\n",
            "Epoch 468/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6431 - accuracy: 0.7495 - val_loss: 0.3215 - val_accuracy: 0.9245\n",
            "Epoch 469/500\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 0.6696 - accuracy: 0.7642 - val_loss: 0.3143 - val_accuracy: 0.9057\n",
            "Epoch 470/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6261 - accuracy: 0.7537 - val_loss: 0.3261 - val_accuracy: 0.8868\n",
            "Epoch 471/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6522 - accuracy: 0.7537 - val_loss: 0.3308 - val_accuracy: 0.9057\n",
            "Epoch 472/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.7516 - val_loss: 0.3332 - val_accuracy: 0.9057\n",
            "Epoch 473/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6574 - accuracy: 0.7747 - val_loss: 0.3316 - val_accuracy: 0.9057\n",
            "Epoch 474/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6107 - accuracy: 0.7916 - val_loss: 0.3395 - val_accuracy: 0.9057\n",
            "Epoch 475/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6452 - accuracy: 0.7600 - val_loss: 0.3284 - val_accuracy: 0.9057\n",
            "Epoch 476/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7773 - accuracy: 0.7095 - val_loss: 0.3212 - val_accuracy: 0.9057\n",
            "Epoch 477/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6995 - accuracy: 0.7389 - val_loss: 0.3246 - val_accuracy: 0.8868\n",
            "Epoch 478/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6385 - accuracy: 0.7853 - val_loss: 0.3212 - val_accuracy: 0.8679\n",
            "Epoch 479/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6139 - accuracy: 0.7579 - val_loss: 0.3252 - val_accuracy: 0.8491\n",
            "Epoch 480/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6564 - accuracy: 0.7747 - val_loss: 0.3111 - val_accuracy: 0.8679\n",
            "Epoch 481/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6841 - accuracy: 0.7368 - val_loss: 0.3211 - val_accuracy: 0.8868\n",
            "Epoch 482/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6258 - accuracy: 0.7874 - val_loss: 0.3138 - val_accuracy: 0.8868\n",
            "Epoch 483/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6260 - accuracy: 0.7768 - val_loss: 0.3141 - val_accuracy: 0.9057\n",
            "Epoch 484/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.7516 - val_loss: 0.3259 - val_accuracy: 0.8868\n",
            "Epoch 485/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6272 - accuracy: 0.7789 - val_loss: 0.3404 - val_accuracy: 0.8679\n",
            "Epoch 486/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6483 - accuracy: 0.7747 - val_loss: 0.3267 - val_accuracy: 0.8868\n",
            "Epoch 487/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6782 - accuracy: 0.7621 - val_loss: 0.3093 - val_accuracy: 0.8679\n",
            "Epoch 488/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7172 - accuracy: 0.7516 - val_loss: 0.3096 - val_accuracy: 0.8868\n",
            "Epoch 489/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7361 - accuracy: 0.7516 - val_loss: 0.3137 - val_accuracy: 0.8868\n",
            "Epoch 490/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.7621 - val_loss: 0.3115 - val_accuracy: 0.9057\n",
            "Epoch 491/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6834 - accuracy: 0.7579 - val_loss: 0.3141 - val_accuracy: 0.8868\n",
            "Epoch 492/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6088 - accuracy: 0.7768 - val_loss: 0.3213 - val_accuracy: 0.8679\n",
            "Epoch 493/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7032 - accuracy: 0.7684 - val_loss: 0.3226 - val_accuracy: 0.8491\n",
            "Epoch 494/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.5855 - accuracy: 0.7853 - val_loss: 0.3128 - val_accuracy: 0.8679\n",
            "Epoch 495/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7132 - accuracy: 0.7432 - val_loss: 0.3136 - val_accuracy: 0.8679\n",
            "Epoch 496/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6559 - accuracy: 0.7600 - val_loss: 0.3109 - val_accuracy: 0.8491\n",
            "Epoch 497/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6063 - accuracy: 0.7747 - val_loss: 0.3068 - val_accuracy: 0.8679\n",
            "Epoch 498/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.6972 - accuracy: 0.7326 - val_loss: 0.3112 - val_accuracy: 0.9057\n",
            "Epoch 499/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7327 - accuracy: 0.7411 - val_loss: 0.3255 - val_accuracy: 0.8868\n",
            "Epoch 500/500\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 0.7303 - accuracy: 0.7242 - val_loss: 0.3238 - val_accuracy: 0.8679\n",
            "Minimum Validation Loss: 0.2890\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dd3jUVfaH35tk0htpBBIgoYYmRUBpCizNtrrq2nVl7evau67+LLjqumsv2F3LKiioKAoqIE1AivTeSYA0SEgvk/v7485MZpJJgSQMMznv8+TJzLee72Tyueeee+65SmuNIAiC4P34edoAQRAEoXkQQRcEQfARRNAFQRB8BBF0QRAEH0EEXRAEwUcI8NSN4+LidEpKiqduLwiC4JWsWrUqR2sd726fxwQ9JSWFlStXeur2giAIXolSam9d+yTkIgiC4COIoAuCIPgIIuiCIAg+gsdi6IIgtE4qKipIT0+ntLTU06ac1AQHB5OcnIzFYmn0OSLogiCcUNLT04mIiCAlJQWllKfNOSnRWpObm0t6ejqpqamNPk9CLoIgnFBKS0uJjY0VMa8HpRSxsbHH3IsRQRcE4YQjYt4wx/MZeZ+gZ26CeZOhKMfTlgiCIJxUeJ+g52yDhc9DYZanLREEwUsJDw/3tAktgvcJun+g+W0t96wdgiAIJxleLOgVnrVDEASvR2vNfffdR58+fejbty9Tp04F4ODBg5xxxhn079+fPn36sGjRIqxWK9dee63j2BdffNHD1tfG+9IW/W05mdYyz9ohCEKTeeLbjWw6cLRZr9mrfST/d17vRh07Y8YM1qxZw9q1a8nJyWHw4MGcccYZ/O9//2PChAk88sgjWK1WiouLWbNmDRkZGWzYsAGAvLy8ZrW7OfBiD11CLoIgNI3Fixdz+eWX4+/vT9u2bTnzzDNZsWIFgwcP5oMPPuDxxx9n/fr1RERE0LlzZ3bt2sVtt93G7NmziYyM9LT5tfA+Dz1AQi6C4Cs01pM+0ZxxxhksXLiQWbNmce2113L33XdzzTXXsHbtWubMmcOUKVOYNm0a77//vqdNdUE8dEEQWi0jR45k6tSpWK1WsrOzWbhwIUOGDGHv3r20bduWG264geuvv57Vq1eTk5NDVVUVF110EZMnT2b16tWeNr8W3ueh2wRdV5YjUxMEQWgKf/rTn1i6dCn9+vVDKcW//vUvEhMT+e9//8vzzz+PxWIhPDycjz76iIyMDCZNmkRVVRUAzzzzjIetr43SWnvkxoMGDdLHs8DFL0uXM2rOeDLHvETbMya1gGWCILQkmzdvpmfPnp42wytw91kppVZprQe5O97rQi7KFkOvrJAsF0EQBGe8TtCDgkMAqBBBFwRBcMHrBD0wMBgAqwi6IAiCC14n6EHBNkEvF0EXBEFwxvsEPUg8dEEQBHd4naAHB1qo0gpdKXnogiAIznidoIcEBlBOANZK8dAFQRCc8TpBD7b4U06AeOiCIJwQ6qudvmfPHvr06XMCrakfrxT0CgLQ4qELgiC44HVT//39FJUEoKWWiyB4Pz88CIfWN+81E/vCWc/WufvBBx+kQ4cO3HrrrQA8/vjjBAQEMH/+fI4cOUJFRQWTJ0/m/PPPP6bblpaWcsstt7By5UoCAgJ44YUXGD16NBs3bmTSpEmUl5dTVVXF9OnTad++PZdccgnp6elYrVYeffRRLr300iY9NnihoANUqgCUVFsUBOE4uPTSS7nzzjsdgj5t2jTmzJnD7bffTmRkJDk5OZx++un88Y9/PKaFml9//XWUUqxfv54tW7Ywfvx4tm3bxpQpU7jjjju48sorKS8vx2q18v3339O+fXtmzZoFQH5+frM8m1cKegUWqbYoCL5APZ50SzFgwACysrI4cOAA2dnZtGnThsTERO666y4WLlyIn58fGRkZZGZmkpiY2OjrLl68mNtuuw2AtLQ0OnXqxLZt2xg6dChPP/006enpXHjhhXTr1o2+fftyzz338MADD3DuuecycuTIZnm2BmPoSqkOSqn5SqlNSqmNSqk73BwzSimVr5RaY/t5rFmsqwOrskg9dEEQjps///nPfPnll0ydOpVLL72UTz/9lOzsbFatWsWaNWto27YtpaWlzXKvK664gpkzZxISEsLZZ5/NvHnz6N69O6tXr6Zv37784x//4Mknn2yWezXGQ68E7tFar1ZKRQCrlFI/aa031Thukdb63GaxqgGsfhb8xEMXBOE4ufTSS7nhhhvIyclhwYIFTJs2jYSEBCwWC/Pnz2fv3r3HfM2RI0fy6aefMmbMGLZt28a+ffvo0aMHu3btonPnztx+++3s27ePdevWkZaWRkxMDFdddRXR0dG8++67zfJcDQq61vogcND2ukAptRlIAmoK+gmjSllQVSLogiAcH71796agoICkpCTatWvHlVdeyXnnnUffvn0ZNGgQaWlpx3zNv/3tb9xyyy307duXgIAAPvzwQ4KCgpg2bRoff/wxFouFxMREHn74YVasWMF9992Hn58fFouFN998s1me65jqoSulUoCFQB+t9VGn7aOA6UA6cAC4V2u90c35NwI3AnTs2PHU42kFATb9cwRoTa9HlhzX+YIgeA6ph954WqweulIqHCPadzqLuY3VQCetdT/gVeBrd9fQWr+ttR6ktR4UHx/f2FvXwuoXhL8WD10QBMGZRmW5KKUsGDH/VGs9o+Z+Z4HXWn+vlHpDKRWntc5pPlOrqQwIJaI8syUuLQiCUIv169dz9dVXu2wLCgpi+fLlHrLIPQ0KujKJmO8Bm7XWL9RxTCKQqbXWSqkhGM8/t1ktdcJqCSOkqrilLi8IQgujtT6mHG9P07dvX9asWXNC73k8y4M2xkMfDlwNrFdK2Z/oYaCj7aZTgIuBW5RSlUAJcJluwcVKrZYIQnRJS11eEIQWJDg4mNzcXGJjY71K1E8kWmtyc3MJtq3/0Fgak+WyGKj3U9davwa8dkx3bgI6MIwwStBVVSg/rytHIwitmuTkZNLT08nOzva0KSc1wcHBJCcnH9M5XjlTlMAI/JWmpLiQkPBIT1sjCMIxYLFYSE1N9bQZPolXurcqKAKAosI8D1siCIJw8uCdgh5sBL2sqHkK2giCIPgCXinoASF2Qa+ZDi8IgtB68VJBN3HzCvHQBUEQHHiloFtCowAoLxFBFwRBsOOVgh4UZjx0a0mBhy0RBEE4efBSQY8GwFoqMXRBEAQ7Xino4ZFG0CvFQxcEQXDglYIeGRFFlVZYS8RDFwRBsOOVgq78/ChWwegy8dAFQRDseKWgA5SoUFR5oafNEARBOGnwWkEv8wvFr0IEXRAEwY7XCnqFfyj+FVITXRAEwY73CrolnCCreOiCIAh2vFbQtSWMoCpZ5EIQBMGO9wp6UAShuoTSCqunTREEQTgp8FpB9wuKIFyVcKS43NOmCIIgnBR4raD7h0QQRgl5RSLogiAI4MWCHhDahkBlJb9AZosKgiCAFwt6YEQcAMV5OR62RBAE4eTAawU9ONIIeslRWTlcEAQBvFjQw6KNoFcU5HrYEkEQhJMDrxX0wPBYACqLDnvYEkEQhJMDrxV0QmMAqCo+4mFDBEEQTg68V9BD2gDgVyIeuiAIAnizoFtCKCcQ//I8T1siCIJwUuC9gg4UBUQSWJ7vaTMEQRBOCrxa0MsCogipFEEXBEEALxf0isAowqoKsFZpT5siCILgcRoUdKVUB6XUfKXUJqXURqXUHW6OUUqpV5RSO5RS65RSA1vGXFesQdFEUcTRkooTcTtBEISTmsZ46JXAPVrrXsDpwK1KqV41jjkL6Gb7uRF4s1mtrIvQNkSrQg5LxUVBEISGBV1rfVBrvdr2ugDYDCTVOOx84CNtWAZEK6XaNbu1NfAPjaUNhWTll7b0rQRBEE56jimGrpRKAQYAy2vsSgL2O71Pp7boo5S6USm1Uim1Mju76TVYImPiCVIV7MmUAl2CIAiNFnSlVDgwHbhTa31cNWu11m9rrQdprQfFx8cfzyVciIxpC8DBgwebfC1BEARvp1GCrpSyYMT8U631DDeHZAAdnN4n27a1KMo2WzQzSwRdEAShMVkuCngP2Ky1fqGOw2YC19iyXU4H8rXWLa+yNkEvyZcSuoIgCAGNOGY4cDWwXim1xrbtYaAjgNZ6CvA9cDawAygGJjW/qW6wCTrFMv1fEAShQUHXWi8GVAPHaODW5jKq0dgqLoZWHaWorJKwoMa0T4IgCL6JV88UtXvobSgku6DMw8YIgiB4Fu8WdEsIVv8golQhOYUi6IIgtG68W9CBquA2RIuHLgiC4P2CrkLaEK2KxEMXBKHV4/WC7h8WQxtVIB66IAitHq8XdBUaQ5xfEdmFUqBLEITWjdcLOmHxxKp8CbkIgtDq8QFBTyBSF5B7tNjTlgiCIHgU7xf08AT80FgLszxtiSAIgkfxCUEHUIVZmAmrgiAIrRPvF/QwI+hRVXnkyMCoIAitGO8XdJuHHkc+a/dLkS5BEFovPiPobf3yWb3viIeNEQRB8BzeL+iBYWAJo0toMduzCj1tjSAIgsfwfkEHCE8g0f+o5KILgtCq8RlBj5fJRYIgtHJ8Q9DD4omuyiNXslwEQWjF+Iagh7clwnqY4nIrxeWVnrZGEATBI/iIoCcQUpFHAJXkFIiXLghC68RnBB0ghgKyJY4uCEIrxUcEvS0A8SqPVXsPe9gYQRAEz+Abgm6b/j84roJZ6w952BhBEATP4BuCbgu5DImvYGNGPuWVVR42SBAE4cTjU4KeElRIZZVmh8wYFQShFeIbgm4JgaAo2gccBWDLoaMeNkgQBOHE4xuCDhCeQGRFLoEBfmw+KIIuCELrw3cEPSoZv6PpdG8bzpZDBZ62RhAE4YTjO4Ie3QHy00lLjBQPXRCEVonvCHpUByjKol9iMDmF5Xy8bK+nLRIEQTih+JagA5d0V3SICeGr1ekeNkgQBOHE0qCgK6XeV0plKaU21LF/lFIqXym1xvbzWPOb2QiijaAHFWZwWmosGXklHjFDEATBUzTGQ/8QmNjAMYu01v1tP0823azjICrZ/M5PJyk6hKyCMplgJAhCq6JBQddaLwRO/gIpkUmAgvz9JLUJQWs4mC9euiAIrYfmiqEPVUqtVUr9oJTqXddBSqkblVIrlVIrs7Ozm+nWNvwtENEO8tNJjg4BYE9ucfPeQxAE4SSmOQR9NdBJa90PeBX4uq4DtdZva60Haa0HxcfHN8OtaxDdAfL20TspipiwQP49Z2vz30MQBOEkpcmCrrU+qrUutL3+HrAopeKabNnxENUB8vYSFWLhmqGdWJ+RT2mF1SOmCIIgnGiaLOhKqUSllLK9HmK7Zm5Tr3tcJPSEvH1Qmk9qXBgA+w5L2EUQhNZBQEMHKKU+A0YBcUqpdOD/AAuA1noKcDFwi1KqEigBLtNa6xazuD7a9ze/D66jY0wfAPbmFtO9bYRHzBEEQTiRNCjoWuvLG9j/GvBas1nUFBL7md8H15LS7zQA9uYWedAgQRCEE4fvzBQFCI836YsH1xIdaiE1Loy3Fu7icJEsHC0Igu/jW4IO0K4fHFyLUorXrhhAbmEZb8zf4WmrBEEQWhzfFPScbVBWSO/2UYzoFs+Xq9P5eVOmpy0TBEFoUXxQ0PsDGjJN6ZlOMaHkFVdw/UcrpRSAIAg+jQ8KevXAKEB726xRgMyjpZ6wSBAE4YTge4IekQhhCU6CHuzYdTBfBF0QBN/F9wRdKeOlH1gDQExYoGOXFOsSBMGX8T1BBzPBKHsLVJTQI7F6UtGBPPHQBUHwXXxT0Nv1A22FzI0kRASz59lziAwOIP2IlAEQBMF38VFBt5UAOPC7Y1Nau0g2yeLRgiD4ML4p6FHJEBrnIuh9k6L4fV8eGzLy+XjpHjxVbkYQBKGlaLCWi1eiFCQNhIzVjk39OkQDcO6riwE4o3s8nWLDPGKeIAhCS+CbHjpAhyGQvRmKcgCY2DuR60akOnYv2eGZCr+CIAgthe8Kepcx5vfO+QAEBvjx6Lm9+Pi6IUQEB7BslxH0F3/axqfL93rKSkEQhGbDdwW9XX8IiYGdc102j+wWz/AucaxNz0Nrzctzt/PIVxs8ZKQgCELz4buC7ucPXUbDznlQYwD0lA5R7M0tZltmoWNbpVXqvAiC4N34rqCDCbsUZkLWZpfN/ZLNAOnzc7Y4tu3JlRx1QRC8G98W9I5Dze/9y1w2902OAuDnzVmObVsOSY66IAjejW8LekxnCIuHfctdNkcGWxyvX718AP5+iq2HCk60dYIgCM2Kbwu6UpB6Juz4CawVLrvOOaUdIRZ/zj2lHalxYWw+WMDe3CL2H5bQiyAI3olvTixyps9FsOFL2DEXekx0bH7lsgFYqzRKKXokRrAuPY8zn/8FgD3PnuMhYwVBEI4f3/bQAbqOhYj2sPQ1l83+forAAPP4PRMj2H9YSusKguDd+L6gBwTCoEmwZ5Fj1mhNeiRGurwf+8ICqfUiCILX4fuCDpAy0vzet8zt7jSnmukAO7IK2Xe4mG2ZBZKfLgiC19A6BD1pIPgHwZ7Fbncntwnh9M4xLttmrjnA+BcX8tzsLW7PEQRBONloHYIeEASdR8GWWbVmjQIopfj8xqE8dFaaY9t36w4CMGdj5gkyUhAEoWm0DkEH6H0B5O9zKalbk8iQ6vz0rZkmL72orLLFTRMEQWgOWo+g9zgb/CywcUadh0QE187iPFpawdr9eVRVySCpIAgnN61H0EOiTW2XTTPdhl0AwgJrC3qFVXP+60u4Y+qalrZQEAShSbQeQYcGwy7WGl74iK5xjtfLduWiteaXrVnsO8ZCXmv357E9U0oLCILQsrQuQe9xNih/2DrL7e7UeLMk3R/7tQfg7vHdmX7LMM7v354jReUs3pHDtR+sYMJLC48pT/3815cw7sWFTbdfEAShHhoUdKXU+0qpLKWU21UglOEVpdQOpdQ6pdTA5jezmQiJNkvT7ZjrdneX+HDWPDaOly/rz9bJExnYsQ2ndmpDv+RoKqs0368/BEBJhZWd2YVur1EfKQ/O4q0FO5v0CIIgCHXRGA/9Q2BiPfvPArrZfm4E3my6WS1Ilz/AwbV1zhqNDg1EKUVQgL9jW1xEEACf/baP2LBAABZscz2/qKySzQcbLsH7wk/b3G6vsFZRWmFt1CMIgiC4o0FB11ovBA7Xc8j5wEfasAyIVkq1ay4Dm52uYwBdp5fujjibiAO8edWpDOwYzevzd7ikNI57YQFnvbyoVjZMzdBM++gQt/e44PUlpD06u9E2CYIg1KQ5YuhJwH6n9+m2bbVQSt2olFqplFqZnZ3dDLc+Dtr1h9A4+PoWyN7aqFPsHjrAkNQY7h3fg8NF5Tw+cyOPz9xIVkEpB/JLASgodc1bL6t0LR3QLirY7T02HpAFNgRBaBondFBUa/221nqQ1npQfHz8ibx1NX7+cPnn5vfCfzfqlPhwI+gDOpql6wanxhARFMAXq9L58Nc97MmpznrJKyl3ObewxsQkP6WaYr0gCEKdNIegZwAdnN4n27advHQYDAOugs3fQkVpg4e3CQtkylWn8sG1gwGw+PsxOi3BsX/jgXzH6yPFZiGNorJKnvpuE5lHXa9f0EIzTzOPlpJbWNYi1xYEwTtoDkGfCVxjy3Y5HcjXWh9shuu2LN3GQ2UJ7F/e8LHAxD6JRIdWx9KvHZ7ieL0+vVrQb/10NSXlVj77bR/vLd7NSz9vd7lO+uFi8opdvXhnKhqo7lhSbmXKgp1UWqtYujOXbbb89tP+OZdTJ//cqGcRBME3aUza4mfAUqCHUipdKXWdUupmpdTNtkO+B3YBO4B3gL+1mLXNScoIU4Fx87fHdfrAjm34YJLx2NdlVAt6Rl4J368/6JikVHNJu9yicgY+9VOd1y0urz/T5aWft/HsD1uYtf4gl7+zjPGS3y4Igo3GZLlcrrVup7W2aK2Ttdbvaa2naK2n2PZrrfWtWusuWuu+WuuVLW92MxAUAb3Oh3XToPz41hHtGh8OmPrpUU6Fvaq0doReMo7UXgmpSsOlby11m+b4ytztfPV7eu1zqjQfL9vLpkakRgqC0DppXTNFa3LqtVCWDxu/Oq7T452yX87sXj3Im1VQxoE8I+R1xcyX7z7MB0t2A66pje8t3s1dU9dSYa1y2b5wezaPfr2BRdtN/nugf/WfrmaYpqC0gokvLWSDU89BEATfp3ULeqdhENsV1n9xXKcHW/w5LTWGW0d34eXL+ju2Zx4t5WB+w2uUZh41g5g1UxsBftt9mNs/X8P4FxeQXVDGsl2uUwFu+bS6Hs3RkgqXfct2HWbLoQJerGMSU3OxISNfVnQShJOI1i3oSplFpPcvh8q6ByrrY+pNQ7lvQhpKKV69fAAAHy3dy4o9R7j41GQW3DfKke5YkwXbsnni241ua65vPniUb9ceYFtmIR/+upuZa+pOHMpzEvRKaxUlthmnwRb/uk5pMtszCzj31cX8+8eWbTQEQWg8rVvQwQyOVhTDrl+afKnz+rVnoJN4D+zYhk6xYUy/eRj/d14vfn1wDF0Twl3O+WDJHh6asb7WtX7ZWj3x6vX5OzmQX0qwxf2fK6+4WtALSisptQ2sBtVxfHOQU2gawNX7jrTYPWqSVVDKzLUHTtj9BMHbEEHvMgZiusCMG2Dvr02+3AuXVIdehqSadUr9/BSThqfSPjrEZZk7Oz9ucl3mLjrUwuIdJlY+tmd1vvtfhqW4vefhourexeyNh7h/+joAQiz+fLMmg6kr9tVbHXLu5kzK3YR96qK0worGXK9myeFjoaTcSn6NcFF9XPHOcm7/7HdZRUoQ6kAEPTAMrpoOobHwxSQoO/Yqis6kxIUx584z+PvornSxleN1pjEzRS8amAzApYM6MKF3omP73eO6M++eM2sdbx+ABXht3g7H65JyK3d8voYHpq8n9aHvKXGTErl0Zy7X/XclL/3cuNCJ1pq0R2dz3xem0Wgohr5iz2G6P/IDGXm1xxTGv7SAfk/82Kj7gskmAlkWsDHM2XiII0XHF0YUvBcRdICYVDjvJSg8BFt/aPLleiRGcO+EHig34t09MQKA+yb0oHf7SAA6xoS6HPPQWWlseGICz118Cqlx1Y1CUIA/neNdQzYAe3KL3Noxe+Mhl/f2RTqmrdzPz7ZegX0m695GLtphr1ljF+jKBjz0ORsOUW6t4o35O2rt23/YVeQP5Zfy0s/bGlzur6iBXP3WTn5JBTd9vIpJH67wtCkN8uuOHLKONjxbW2gctddca610GgER7eH3j6HvxWbAtAVIig5h9zNno5Ti1tFdKau0EhTgz9ZDBUx4aSHXDkshwN+PcFtaYnKb0AauaFZEsuNcaqDmJKVJH65gaOdYlu7KBWDPs+c4Zq1a/Bt+3sKySsa9sMBlW82QS2mFlbLKKkdefqKtGNmny/dx0anJDOzYptZ1j5ZW8PCM9Ww+eJSd2UVM7JNIWmIk2QVlaDQJEcEuIaHW6qEfyi9l5toMbhjZ2a2zYMdeP2iN0/eisby3eDdj0hJcHInGUlph5YHp63hgYlqdVUVrcsW7y2kbGcTyh8ce8/2E2oiHbsfPD4bdBrsXHPfs0cbi/M9or7veIzGCtY+N57Fze7kcm2DLda/pxTuzel8eqXFhhAcFNOgx28UcIKewzOFpV1RpKq1VtXLajxSVc/1/V/DWgp2s2ZdXq5GorNKUVli54/Pf2ZCRz9mvLGLAkz+SkVfCmH//wnqnXPhNdVSU/OjXPXy37iA7s01Pw16xcvDTPzPkaVPm2HmcoKag5xSWHdMYwLHyz+830/PR2bUE8kBeCdNW7q/jrIbZeqjgmFa+uvmTVfzz+y0N9qYKS903eJ8s28vlby+r87zSCitPfbeJP72xpNE2OfPz5ky+WXOAp2dtbtTx9r+ZPX33RHEgr4Rdx7FAjTcggu7MkBuhbV/47q461x1tSaJCLfj5uXpefn6KL24eype3DK11/HMX9XW8Dgvyp38Hk2HTvW24y6QngM7xYS7HA8zbkuUQ9FnrDtL1kR8Y/2L18nqlFVbeXbyLnzdn8cwPW3hn0a5aNlRYq5i2cj/frDnAU99tYld2EVUaZm84xK6cIpburG5A7L2BX3fmuMTzc2vEev88ZSl7clzDSEdLqwdPnRsVrTWDJv/MXdOatoh3VkEpN3+8yuU+dt5euIuSCisXvL7EpVGa+NJC7v9ynWNhkqyCUsa9sIDdOe5DYNYqzbwtmWit+WlTJhNeWsi36xpf9sgemrA20AgUllU/g3MD/Y+vN7B0Vy4Fbp4RqudDOGdNHQv23pq/X+N6t57qaQ17dh5j/lPd05yxOr1R80a8ARF0Z/wD4NwXoDgH3hkNWVs8bREAg1NiSIioXUf90sEd+fT60wA4XFjOGd3NotYBfn6OAdkY2+IcCkiNc42/3//lOseyenZ25xQx6cMVfLv2AD0fm83r86uXzFuwzaRSOs+K3ZtbzGSbR/b7vmoPdsZqU74gq6CMiOAAQgP9+feP25i6Yh9XvLOcy96p9hQPuxm8u/3z3x2v3120iw+W7HG8LyyrZENGPjd9vJJDNpGb1YAw/rb7MFsOHUVrTfqRYqpsPRI7r8zdzuyNh/hqdXW+f0m5lV93uK5MdcSpsNpRmydsF6ZNB46yPavQpfqmnWkr99Pl4e/564creWvhLu77ci1QPdBbF4VllY7U0HKbvQ0J4VEnD/2f39f2lndlu29wyiqbNjZRYTWCHtCI8B1AUbnrc9w9dQ1fNKHHczwUllVy97S1XPt+3eMNy3bl1lpNLL+4oladppMBEfSadBgCf3jMvM5Y5VlbGkGf9lEA9E6K4tLBHQEYk5ZAUZn5AnZzynvv7JR1c8+47o7XCTW8+V+2ZnPbZ79TlyNY0/uPCQ1kbM+2DsEB1wU7Qiz+RASb4ZoHppuce+e4/wE3GTDrnCpYTp61mc9+2+d4X1xeydKduczZmMmcDYdqnQuwLbOAn2wDv1VVmkveWsrElxbx8tztjHhuPhdP+ZWuj1QPgNu9S3sPad6WTHo+Npsr3nWtxvn+4t2kPDiLQ/m1xyqybKEDe8jj1x05DJr8Ex8v3cP9X65zHP/sD1scXnBYYMc9Tl0AACAASURBVN2Tv7TWDHtmLhe+8Ssl5VZHiKJmjf2a2O8fYvF3fI7ZBdVhjV05phHJKSxz6SmVVTQtbGWtMucHNOCha23CdPbvqH3bjN8zuO/LdW6zsWqyeHsOKQ/OapJnXVxeycO2OSCZBe4HZvfkFHHZ28t47BvXJZX/+PpiRv5r/nHfu6UQQXfH8DshIAQy3a6L7XHm3zuK724bAZgwzVd/G8Z/LulHVIiFDU9M4K5x3R0LbXRva7JqlFLEhQfx1Pm9uWhgMreM6sLGJybw4aTBLHpgNPPuOZOHz05jx9NnuTQCAGN7tnV5bxfnTrGhvHxZfz65fgjDu8bWaW9IoH+9Me66PMa6KCqzOsIGzj0Me22cg/kljH9xITd8tJLCskq2HCpwHPPGL6bHsdrWmyitsDJ/axaFdnHRmoy8Ev76ofsac3O3ZAEmXGXHIegFrqtWTVm4i5zCcpdxi5oE+Lv/F8wqKOWq95Y7vO2cwjJHg7k3t9htL8COXfBP7dTGURxu/5Fqb9L+eQ+a/DMXvF4dL3dukI8ltu843/Y39vfzc4yr7MgqqHXce4t3k/bobPY5ebjOYZ6ej81u0FOfYltsvTHr+NbFB0v2OCaq2RuhF37c6jJZzt4jc/4OQXVW2MlW+kIE3R1+/pDQEw783vCxHiA1Low+SVGO9wM6tiEy2GSVhAcF4O+nyLf9g3Rva8TZ7jNdPTSF/1zSjwB/P8KCAhjVI8GRDnnjGV0I8PcjLrzaA79vQg8eOjvN6V7RRDjd6/z+SXRNiKBXO5OCGeu0/mo/W0w/xOLvEvcOD3JNrqoZQ2+IorJKR9Gz3/ZU17h54ttNzN+SxaQPqrvPGUdKmL+1WnxrNixLduQw6QMTYgKT8jf82XkN2vCt04zVGavTqarSZNm84ILSCmasTmehLURVM6zlTHEd3vbzs7eyZIfrALbd9odmrOecVxbXeU27h969bQSZBaWUV1ZxuLD6M3513g7G/PsXALZmVguVs4d+tOTY49v2RrGs0soNH63kmzUHaq0HADDDFtba7iT2B2p42vd9uY784gqmrdxPyoOzSD9S7DKQmWNbzMXf79gkzDkl1jl05e+nqLRW8cq8HVz4RvUEQ3sZjbrmj9T13dVaH1ej2FRE0Ouix9mwbyl8fCFUeV/es92z62LLWz+WLEz7GqqXDErmljO7kBhp4vepcWF8efMwImyCHOYkzD1tOfVdE8KZfedI1jw2js621Ldgi79LAbJLBzsvcOXKnWO78fzFp7hsS25TnQKnlMlDr7l2q51JH65gy6ECkmxpcwu2ZfH2QtfB3E+uO83xumbmTU5h4xoXZ6/7rYW7+GzFPkfIpaCs0nHPPkmR9V6nrvBJXo0ZtDmF5dRMYKrrXHtj171tOFrbVrMqMraF2Or77HIauC22xbKdPfQd2bU9azuDJv/EQzPW1dpuF8hv1hxwVAVtG1l77Mf+XXRuNLIKame67D9SzFs2T/zM539hzH8WOBo1u5A6D/BqrRucuew86O2cERbg5+f287QXvqsrjFRzRTI7f/lgBRe9WXvm+ZyNh7j+vyuYXUeosKmIoNfFoEnm9865sOA52Hnyxcvq48IBZp1uuzjbPfjGYPegO8WG4eenCAsK4PUrBvLZDafj76cc/0wjusY5zokMtpCWGEGPxAjSEiOJDg10xOZDneLEY9ISuGNsN8CUAE6MDCYltjol844/dOPPgzowuocZeJ3xt2Esun+0Y390iIXi8kq3qXltI6t7FtNvGQbAP7/fQn5JBef0befYN7xrLDP/Phxw9VDBpBI6k1RPPrXz/XZmFTnisAWllRzIK+GaoZ0YnBJT6zz7s0FtUV66M5cPl+x2xP/tTJ61qdZ1DroZewDjoYcHBdDBluqafqTE0VB1djN72R5OKHMa+Fu26zA73aT2aa3JKSzns99qh0Tsz+IsqiVO16yq0vyyNcsxNuM8ocj5tf3z2Xe4GIstJGW/5vLducxad9AxJuD8PXhr4S66PPx9rUHjlXsOO453Du3U9NCdnYSr31vucnzN7LNqu2s3RFkFpSzcls3qfXkOLz0jr4Tyyio+WbaXnzdnsbeOyYBNRSYW1UVYHDyaC6+dagQd4MZfoP0AT1rVaP518Sk8fn5vIoICuGdcdy4elNzoc+3fXWchPueUakG8YkhHDuSV8NcRqS7nTbt5qEud9gSbdxZi8efx83rx3pLdvG9bl/WHO0YSGxZIfEQQSil+3pTJxgNHHTn6L18+gK9/z2BAh2iXvP3QwACyC8ooKKugQ0wIQQH+ZB4tpaC0khtGdmbyrM3EhpnGxOKvqLBqLhqYzJi0BGatP8iAjuZ69oHd9TVqxq9wCuF8d9sIvl13gLcW1E7XBBOjtodTCkorHP/cq/ce4WhpJe2iQhwe5MuX9eeOz01q5QeThrA+PZ/zXlvMp8v3kZYYwdVDU/h93xEuf8d9nri73PNzXlnMaZ1j+Oef+vLUd5v4zyX9iAi2UFhWQXhQgGNyzxu/7HB4zO2igl0GrAEufONXPpw02CWs8PycrTw/ZyufXHcaI7pVN9z11d6p2WtqHxXsKD+gtWbyrM28bxvnABwZSuAqjH2To5m/NZupK/bXil3/sjXbxSt3vud/f91jrlVQBgVlvL1wF0+e35uLpywlKTqEiwYm0dspVOmcXRVQQ9Dtn5f9eesa53U3mLpwW3VmVFZBGYu25/Do1xu4a1w31u7P4/IhHbiuxv9OcyEeen34B8A1M2HQdeb9/H/Cb+9A7k7qTAE5SQjw9yMy2IJSitv+0I12UY2buQfV8cK6HrFjbCivXD6gViw8MtjiUrLX7qEHB/pz7fBUFt0/xrGvZ7tIEiKDHWI9tldbh+duv9Y1Q1NqzYgck5bA7I2HWLX3CJ3jwvn57jMZ1sUMyHaOD+O1Kwbw+Y2nO3oW5l4RDEmNoV9yFM9dZMI5sWHGtppCae+Gz793FH2SohzhJXd0TYhwvC4orXR4gfZwRvvoYG4d3ZX/O68X557SnsEpbbjbll3UNzmKSNvg8qPfbOSeaWv5ky12+49zejZqpma5tYpF23N46rtN/Lgpk/lbs1mfns+69HxiwgJpZ5ulaxcngKA6Sipf+8EKVu01g4HRodW9uaveW85ip/MP5tc9Td85/z0wwI/kmFBHb+6LlekuYg64ZAo5i3tsWCDRoRZHmqydyOAA1uzPI6ugzDFmk1VQ6vCC7fnvOYVl3P7Z73z22z5+tc2DyMgr4ZV5OxyD4lBdxgKMB76sxuD1e4t3O0JftgQe1qXnUVJudYRg3E2Kcl5Y5l+zt3LvF2spqbDyw4ZDHC2tpH+H6DoHw5uKCHpDtOlkctOH3wHbf4Tv74VXB8KCf530on68pLUzQtWhntmpjaGtk4feVH6++0xm/n04141IRWsorahyZNs8dl5vrjq9IyO6xnPuKe3pZsvsefy83gAM6xJHfEQQ3/x9hCPrJzDAzyVkAtWx+vZRwY5QS5gbQe8YE0psWCDnOvVaNh866hKDBmgXFUJYUACThqfi76f44uZh3P6H6kbLOV98ui1vPyzQn+tHdmb+vaMc+4ICzL/pnWO7ufSA7Nirdb700zbOe20xWw4VMDotnmCLv8sANwD1fGVfnmsGMO+f4FoR9Kr3lpPy4CxenbvdRXhr4hwHjwy2EBMa6PDQt7vJdnG+1qfLq9NSE6OC6eCm5MWFA5NZtfcIv2zNdpSUeGfRbj6xnWsX2W/WZDg865oT1OpKl92RVciT37mGtZ76bpNjMLmgrJKcwjL++NoS7vj8d0fDn3GkhCkLdrpkC23IyHc4M/a/q/O9O8Yce1mFxiKC3ljOuA/6XAz9roD4NPjlGXg6EfbVPZXaW7liSEe+vHko43q1bfjgerB/qZtD0LsmhHNKcjTJbUIcdWfsgp4UHcLkC/oSGOD6db5gQBIbn5hAr/buByb7JbsuPPLq5QPY8+w5LHlwjONap3Zq49hnn6Q1JDWGVY9WD/qC+5BIh5jG94rsvHxZ7ZDe2v8bz/anz+LOsd1rNRrO2HsG8RFBXGir2OkcNrtwQBJVdTghF/Rv73g9rEssT57fu9Yx//lpG1+uMgJlD0Hsyi5kzsZDXDJlqcvEssiQAGLCAzlcVE7m0VJ+232YjjGhjl4JmHTPmnn4141IZXyvtm5LXTg3oPFODZU948juoX+ybJ8jJbK+9M5sNwOxNbFfZ/PBozzxrRF853LX+48U8+wPW1izP4+iskrKKq2sz8jnnFPa1Rp/sQ8txIUH0lKIoDeWoAi4+D3405um3O7QWwEFG2Z42rJmRynFIDeDecdKQmTtQdGmEuDvR6dYI6QRjRjodedh26lZ+KxznD0jqDrMc0pyNL8/Oo7z+rV3NEx2EXLXbbaHoX59cMwxhbkAJvRuy1inRvS3h//AovtHE2zxdwwOXtC/PZ3jwxzpoWN7JvCvi05xiEfn+DBWPDLWkd1kz+r4+e4zeeHS/rUEPTI4gMAAP0fZCDC9l5qTzezYZ+SGBQagteaqd5dz08er2HLoKA+dlcYZtlnEkcEWYsMCOVJczunPzGVtuvFaE2pkvfSvsZrXhQOTXMY47Lx0aX8GpcQ4QlYhTt+p33Yf5oEv17ntMNccLwBqLTJTH86VTL91s7jKeqcJcGNfWECPf8ymrLKK4V3ieOqC2o0iQGzNXlMzIoOix0NUMkx4GnK2m+qMA6+GxL4Nn9fKCA0M4K/DUxmTltDwwcdA57gwdmQVOio6Hi9/H9OVyJAAxvVqy/Jdh4kKdX+9NjbxDAsyIhJaTyMx/ZZhJEYG13ktZx4/rxePf1vdzQ8NdL1uTfEDeMnmwU98aSG5ReVcODCZs/u2Y+Xew0xbme7iuQK8ffUgth466hCxoZ1jmbOx2sP8/o6RxEcEuXjXQQF+RDbw2ZZZq9iaWcCB/FJGdovjxUv7Exce5BhkjgyxkNwmxCXVMtzWeDiXO7hkUAeXfHv7Z+Bc/TMuPIgLbFlb9jCZcwkGgKkr99PGzWfuTtBTYsMaLLlgp74xg57tIl0mNjkfO6RzjNvSAH7KZGq1FOKhN4VRD4KugvnPeNqSk5bHzuvFaZ3rnkV6PNw7oQf/OKcnl9WTz94YYsICuXNsd3q3j6qVseMOu5cc6iaE9MG1gxnQMZrUuLBGiTnAtcNT6ZdcnXVxLBNR7HF1+wCmvddSc6B6SGoMVw9Ncbz/y7AU3rhyoON9dGggQQH+Lh5xkMXfpbH8cNJgl2ue2T2e8soqfrI1DP/+cz9HrN7ei4kIDqg1BnOkuMIxp8GO8wIuUN2bs0+cCwzw462rq+0d1yuRLvFh3HRGl1pefF5JhcuAbl10ckqTffTcXi5hIHc4p3qe4vT3+kMdjkpUiIXIYIvbHPyYsMA6UyCbAxH0ppA0EAZcDVtnwbvjYNa9XjkJydvo3jaC60d2btGuqzvCbN6jc3f/fzecxs93n8notAS++tvwWnH8hnjo7J6O18eyml8Xm8dtF3C7eJY2UGBLKUXPdtVjCvbGKS6s+rMMtGVI2RnVI4HLh5g6QY+f18sRVvlu3UF6tYt0ES57llNksMXRyNiJDrG4pL8+dX7vWguZ29//sV97vr51OFufmsipnarDfzFhgcy9ZxS92kfyi9PAMZgchXvG9+CT607j/ok9HIJ/Zvd4h82Ay7yHPw9KbrAHOap7AtNt1U4fmFg9YHzt8BTHa+cQlT2TKia0dqw8Nqxlv7MScmkq/a+AFe9A+m/mp00nU1dd8DmGdonltz2HXZbTG9Ylrp4zGub0zrGsfnQcF0/5lVtHd230eU+d34eR3eLoa/Nk7aGWmvXq3dEpJpS7xnanV/tIh7cYGVItBRZ/VauXYfd8Y8ODHDNLt2YWcOvoLi7H9U2OIizQn9NSY1y88ecu6svotATiw4O4a2x3JvZJpIdt9a5Zt49wlDKwe+hKKZe4vjvCggK4ZVQX3nRKRewYE8qIbnGM6BbH4u05ZBeUcd2IVM7oHk/Kg7PM8zs1NJHBFke20QuX9OO1eTvYlVPEo+f24ilb1kt0qIVTO8WwbfJZjgY72GJKZPRsF0lSdDAvXzaASqt2+dxqeuJJ0SGOgfWWQgS9qSQNNBOOtIZF/4Ef/wHZW+D81z1tmdDM/GVYCou2Z3PJoKaFemoSExbIvHtGHdM5YUEB/GlA9WSxXu0jCQzw4++NaBT8/JRLzj+4DgQrpQi39Ubsg6+3j+lG24ggzu7bjh82VJcqHt3D1bu9ZFAHl8/n+hGp9EmKcsTAgVr37t0+ijO6x7NwW7YjrNVY7p/Qg8hgC8/NNqWunbNj3r5mEAWlFY7B6fiIILILylxCLlA9vT8pOoQJfRJ585edjrkNUN2Y2cV88QOjHT2J728fUe/qUTP+NsxRG+b+iT2Oacb28SCC3hzYZ4+e9wrk7oDfP4GR90BAMES2r/9cwWuICQtkxt+Ge9oMt0SFWNg2+axmu56fn2LKVQPpbSvPHGKbHAaumUMNedH/qLECV128ffWpx7WwhlLKpdaPc6pgeFCAy5jCtJuGsmLPYUeozj4nIKlNCCv3HiEhMpi7x3VnQu9El7BUzcF35+yo+sQcYGDHNiREBJFVUMb5/ZPqPbY5EEFvTsJi4bL/wWuD4RWbyF/0nlmj9NAGyNsHaWd71kZBaCQT+7Rzu31ISgzXj0hlXK+2zTbjMdjiT2LU8aW3OlcerW8MIzUuzDEDd+MTExzhnaf/1Jc/DUhy7LM3Up1iQ9mbW9xgxk9D/HTXmRSWH3v1yuNBBL25ie0Cf51t6r/s+BmmX2d+7FzxBXQf7zn7BKEOlGrc5OewoIBGe94ngtS4MD674XS3ywfWhXMvI9xWRrom/ZKj2ZtbXGfp3MYSFWppdOZTU1GeqNkLMGjQIL1ypftFBHyGvH3w9mizpF3XsZC5EYoPw00LISGt4fMF4QSSdbSUvJIKR3mE1k5+SQUfLtnDraO7tFjtleNBKbVKaz3I3b5GWamUmqiU2qqU2qGUetDN/muVUtlKqTW2n+ubarRPEN0R/rbMDJBe8QXctAgsIfDGafDTY1DeMiU0BeF4SIgMFjF3IirEwh1ju51UYt4QDVqqlPIHXgfOAnoBlyul3PW3pmqt+9t+3m1mO72X8HgYcBX4+ZnX454w25e8DJ9dZhaiLi+Cpa/D0QNm9mlhVv3XFARBcENjYuhDgB1a610ASqnPgfOB2hX3hYYZcA2ExUPefpj9oPHWgyKh7CjMedgc4x8II++F024CSyiU5kF4806fFwTB92hMXyIJcF6eJN22rSYXKaXWKaW+VEq5TdRVSt2olFqplFqZnZ3t7hDfx88P0s6B02+GuzZCykgj5p1HwanXQpcxYC2HX/4Jz6XA5Hh4oZfx6KucKu0VZrkWBisvhuVvQ1ndS4cJguDbNFeWy7fAZ1rrMqXUTcB/gTE1D9Javw28DWZQtJnu7b1EJZm0xt0LjMgH2mawVVXB9jkw72kzSamqwsTcF70AI+6E3YvM0ngAMakQ2w1m3QPrPoe8vdBpOPQ469gWEhUEwetpMMtFKTUUeFxrPcH2/iEArbXbilS2mPthrXWUu/12WkWWS3NRmg/vTYDszfUfZwmFCluFt3FPQtdx0NY23LFumkmp3LMEkk6FFNsEmfx00yNokyoNgCB4AfVluTTGQ18BdFNKpQIZwGXAFTVu0E5rbZ8P/EegAeURjongKLjuR6goMSGVmM7Gq8/cCHOftJXvPQUS+5gwzNLXjEf/02OQNMjMWN272PWad66HDdPh58fN+3b9Tf685dgXZRAE4eSgUXnoSqmzgZcAf+B9rfXTSqkngZVa65lKqWcwQl4JHAZu0Vpvqe+a4qE3E5XlEFCj4E/mJkhfAYfWm8JhzgRHGY8/6VTIWAVp50LyICPsff8M7fpBxmrocxH0PNec89s7ppzBuCeh85kn5LEEQXBPfR66TCzydfavgPICSB4CK96FgdfArLth41cQGAF3roPQGOPNL3m5+rzACLh/lykN/MUkQJvJUVdNd3+folzY/QukjjIlEARBaBFE0AVXtIYDq41ox3ev3p6xyvzO3Agzb4NLPoJv7zQhnqRTTYMw8Vk49S/w5V+Np58ywiyebT83KBIm/WDCP/Xd3x6vP7TBTMAKCDY9jfJiCAgCv+Zbtk4QfAkRdOHYsFbA811N/jvAJR+bzJnp18Gu+RDXA3K21j7vtFtg7f+gbV+45hvwdxqi2fqDCetkb4Wuf4Arv4SV75vegp3+V8GaT6H3BfDnD6Ekz6wIFdr09U0FwVdo6qCo0Nrwt8BpN8OqD00qZdc/mN9XfwX/7mbEvOd5cNH7UHIYcndCp2HG627bG2b+HeY+YUoI//qqibt/dbMRZ7QpWvaEreyqn8WkZQKs+cT83vqDEfP3xpvZs3+dXdvj1xoKMyEi0dXjr0l9++piyyzTa5B1YgUvQzx04djYMN3E3899ue5Y+Xd3Ge87LAGKnMoYXD/PCP4P95v0yh1z4eoZ8N/zTT59YLgR0rX/c71e3z/DRU7VJLSGr24yqZhgzu19oVlsJO0cs62s0DQs2+bAH181JYydsVaasYWQNq7b1/wPvr4FojrCXevr/yw2fQM/Pgpn3m/KO9RH+irTKAWc2GXzBN9DQi7CiaW82HjXVRWQ0NM0AJFJZmass7ds957Li8AvwIhdZTks+reJ46edAwfXwvIp0G2C2RYQZDJwFv7L/b3P/jf0ugAWv2DOC4qEylK4eTHEdYPKMlM3Z8nLJqR04buQfKoZJ6gsg1cGwtF0cy37LN6B18BgN/Xmpl4Fm781tl8xzfRk3LH9J/j0YkgebMYcEnqZRmr1R1CcCyOdwk7WCjNW0Wk4tDvl+D7/utDapL4GhjZ8rHDSIoIunHisFUboig/Dsteh4zDoNvbYr5OfAS/aJkcl9jXvSw6bvPlrZ8Haz4z4zX8atnxnjus03DQEaefA2MfhjaGg/GDIDWYA98Dv9d/z/Dfgl2cg36niRefRcMZ9ZkKW1jDjRlg/DXqcA1kb4cge05CMfsR1oHndF6ZGT1GNgmux3SB3u3l98xKI7wGlR+HXV2DJS8be29eYNWprUpAJOdvMc/rVU71j24/m3mlnm/GJmbcZO//+m0lfBdeQVGGW2X4svYjyIlj8kmlkpST0CUEEXfBuDq2H0FiznF/uTiN4Z9xnwjPO7FoAv71thD0sAa6bYzzv3J3w+ZVmpm1EOxh2uxHYDqebSVh7Fpnz26Qa7/myT83s2eLDZknBXfNh5QemIblgipnUtfYzc87VX0N8mhnwXfe56RFcPxfC4uCj8+HQOmiTYlayKjho7jH3Sdj0tZn0lbUZOp5mZvlumWXENWWESTdN6Al/mQlBEXB4F0y7xjz3yg+MTeMnu1+QvKIU5j1lns0dEe3MdQZfB1/fCvuXmfkHG2wpqSPvNdetqjTP4UxpfnVjAGaM5Md/mNcXvmuuYwmB6Casu3p4t/nb+vmbXtnK941Nke5XUDphVJbB3iWw5jOz7OTQv7nuL8qB6debEFynYS1mhgi60HrI3QlzHjGeubPHWGU1ohjTpbZXe/Sg8drrWx6wrADeGWM8YzACM/oR12sd2gAfnmNCOX4BRhC7/AEu/9x18pe1wgh5Qi/47a3qKpvdxoPyhwvegL2/wtQrzfbUM2H/b1BZ4mpTu34mHHTgd3O9TsNMY/XtnWb+wOAbzGSwTV8b0b3oPSOOm2ea852zlfwDoePpsHth9fX9LDDqARhxtxHXXb+YRmric9D/CtMA/fAAlOW72uUfZMY8IpNMbyVjtZm8FhhmBtr3r4CznjUNlZ2DayGhNyx7A356FPpfaXo8X91kGtKgKDPRrSjb9Ga6jYfPrjAD7sW55u97+WemsfE/xtWBts0x4bC6sqmKcs3nueg/podjp/tEUzm1bR/oMBjWTjV/z9A4uHe7+W6krzI9uD4Xm1DXth9h/RfQ50JTb+k4EEEXhOYgd6cZ8B1+R93x8m0/wtc3G5HpNh6u/KL+a2ptBKWyFHr+0bWBWPKyifcXZpp5AB1ON+GrM+6H8kIjfnbiulc3NmCEfPgdrvdRyoRVVrxrwjt7F0NkMlz4FljCTJrpofVmpvH6aSYbCUyjdPbz8PGfTPE3MA2PtppG6YppsOFL463H9zQCnGWrrq38bNlNbohMgrOeg00zzf3cERYPYx41obJts00jCWYwu+SIaYis5WZbdEdTljppoEm1jbIVhc1YDVHJZhC753mmyN3RDLj0E5NR9fnlJiT41x/M8daK6kahrADeHF793Be/D1EdzES8fUtr2xscbRr0xL4QEmN6f7rKiPwpl8Daz83nM+ZROONe98/cACLognAiKS82nm7nM5unNk7xYSMUShkRC40x+fw/PQYdToOQaDh1krnn9/caj/Gid5s+Oaus0Ewg2z6netuoh01pZ4Brv4eOQ2v3eEqOwPovjTCu+RQyN5jtQZEw9O+m5/TN382Ac026TTCe9m/vmBDZqIdMaiqYQnJZm+G7u42nfu6L0GGI2bb0NSOwsd3g8E7zufgHms9g5zz3z2dfh8BO13FGfPcthT//12RB/fCAuVfqmSYM1W2cOdZaaXpM+RkmzFJVYWwZ+wS8P8E0/lHJptcU3wM2fm16IZYQuGamCdn4H1/WuAi6ILQWjifvviGyNsO6qUYke5wFW2ebGHnb3o07v6zAxPXD46u3FWZBwSGTJRTZ3ghfVaUZj2ioIdLaCK/zcbk7zcLsp90M8/8JO34y4y7Fua7nhsRA+/7mPlWVEN4WBlxtwim/vVXt+Tsz9O8w4enGPSuYhqzKCpZg1+356WZGdM1xiWNEBF0QhNZDwSGzQHuHIeZ33j4j7nE9AG3CQO4avbJCE+cvyoaFz5tVwnpfaMpOn0TITFFBEFoPEYnVYZrojrWzoeoiIpk4BAAABGlJREFUKNz8Dk8wYwZeiPcsZy0IgiDUiwi6IAiCjyCCLgiC4COIoAuCIPgIIuiCIAg+ggi6IAiCjyCCLgiC4COIoAuCIPgIHpspqpTKBvYe5+lxQE4zmuMNyDO3DuSZWwdNeeZOWut4dzs8JuhNQSm1sq6pr76KPHPrQJ65ddBSzywhF0EQBB9BBF0QBMFH8FZBf9vTBngAeebWgTxz66BFntkrY+iCIAhCbbzVQxcEQRBqIIIuCILgI3idoCulJiqltiqldiilHvS0Pc2FUup9pVSWUmqD07YYpdRPSqnttt9tbNuVUuoV22ewTik10HOWHz9KqQ5KqflKqU1KqY1KqTts2332uZVSwUqp35RSa23P/IRte6pSarnt2aYqpQJt24Ns73fY9qd40v7jRSnlr5T6XSn1ne29Tz8vgFJqj1JqvVJqjVJqpW1bi363vUrQlVL+wOvAWUAv4HKlVC/PWtVsfAhMrLHtQWCu1robMNf2Hszzd7P93Ai8eYJsbG4qgXu01r2A04FbbX9PX37uMmCM1rof0B+YqJQ6HXgOeFFr3RU4AlxnO/464Iht+4u247yRO4DNTu99/XntjNZa93fKOW/Z77bW2mt+gKHAHKf3DwEPedquZny+FGCD0/utQDvb63bAVtvrt4DL3R3nzT/AN8C41vLcQCiwGjgNM2swwLbd8T0H5gBDba8DbMcpT9t+jM+ZbBOvMcB3gPLl53V67j1AXI1tLfrd9ioPHUgC9ju9T7dt81Xaaq0P2l4fAtraXvvc52DrWg8AluPjz20LP6wBsoCfgJ1Anta60naI83M5ntm2Px+IPbEWN5mXgPuBKtv7WHz7ee1o4Eel1Cql1I22bS363ZZFor0ErbVWSvlkjqlSKhyYDtyptT6qnFZk98Xn1lpbgf5KqWjgKyDNwya1GEqpc4EsrfUqpdQoT9tzghmhtc5QSiUAPymltjjvbInvtrd56BlAB6f3ybZtvkqmUqodgO13lm27z3wOSikLRsw/1VrPsG32+ecG0FrnAfMxIYdopZTdwXJ+Lscz2/ZHAbkn2NSmMBz4o1JqD/A5JuzyMr77vA601hm231mYhnsILfzd9jZBXwF0s42QBwKXATM9bFNLMhP4i+31XzAxZvv2a2wj46cD+U7dOK9BGVf8PWCz1voFp10++9xKqXibZ45SKgQzZrAZI+wX2w6r+cz2z+JiYJ62BVm9Aa31Q1rrZK11Cub/dZ7W+kp89HntKKXClFIR9tfAeGADLf3d9vTAwXEMNJwNbMPEHR/xtD3N+FyfAQeBCkz87DpM7HAusB34GYixHasw2T47gfXAIE/bf5zPPAITZ1wHrLH9nO3Lzw2cAvxue+YNwGO27Z2B34AdwBdAkG17sO39Dtv+zp5+hiY8+yjgu9bwvLbnW2v72WjXqpb+bsvUf0EQBB/B20IugiAIQh2IoAuCIPgIIuiCIAg+ggi6IAiCjyCCLgiC4COIoAuCIPgIIuiCIAg+wv8DljtC/qfF5N8AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "ID0-l5vV7Rzf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM classifier using using linear kernel \n",
        "from sklearn.svm import SVC  \n",
        "model_svm = SVC(kernel='linear') \n",
        "\n",
        "#trained svm classifier\n",
        "model_svm.fit(y_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "LCN2mVXP8l3M",
        "outputId": "8d17824f-e647-4144-970a-eba9f26d5680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-927163db2709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#trained svm classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    991\u001b[0m         )\n\u001b[1;32m    992\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     raise ValueError(\n\u001b[0;32m-> 1039\u001b[0;31m         \u001b[0;34m\"y should be a 1d array, got an array of shape {} instead.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (528, 11) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary packages and libaries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LN_fAm9D8oYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6Y7nj2qC03B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
        "sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "zQ6N5AesCqaM",
        "outputId": "79df0d3c-85b4-419d-f77c-0e3602e4587e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-984c8e56b837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'poly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    977\u001b[0m     )\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    991\u001b[0m         )\n\u001b[1;32m    992\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     raise ValueError(\n\u001b[0;32m-> 1039\u001b[0;31m         \u001b[0;34m\"y should be a 1d array, got an array of shape {} instead.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (528, 11) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "5TooOF-tC4M_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn5 = KNeighborsClassifier(n_neighbors = 5)\n",
        "knn1 = KNeighborsClassifier(n_neighbors=1)"
      ],
      "metadata": {
        "id": "La5CcJt2EAt9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn5.fit(X_train, y_train)\n",
        "knn1.fit(X_train, y_train)\n",
        "\n",
        "y_pred_5 = knn5.predict(X_test)\n",
        "y_pred_1 = knn1.predict(X_test)"
      ],
      "metadata": {
        "id": "mAhv4087EIOw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy with k=5\", accuracy_score(y_test, y_pred_5)*100)\n",
        "print(\"Accuracy with k=1\", accuracy_score(y_test, y_pred_1)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VYsDkfpENSp",
        "outputId": "ad418aaa-6fed-4861-ad46-6d8f809a15ef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with k=5 56.27705627705628\n",
            "Accuracy with k=1 56.27705627705628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQGrJ_8tFoCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3v5N1CdZW-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yH3OBxRnZW6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "UGyP1WI2ZW3_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train, X_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "AL1ziijaZW1R"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "# define the model evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=11)\n",
        "# evaluate the model and collect the scores\n",
        "n_scores = cross_val_score(model, x_train, y_logi, scoring='accuracy')\n",
        "# report the model performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "bQhXX-l2fHq8",
        "outputId": "cc6dc174-5745-450d-a390-509ac17fcdb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.108 (0.031)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDjfErD_gFxL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}